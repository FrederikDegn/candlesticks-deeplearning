{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "4cd7ab41f5fca4b9b44701077e38c5ffd31fe66a6cab21e0214b68d958d0e462"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Model Exploration - Convolutional Neural Net"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "### Package Setups\n",
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "from tensorflow import keras\n",
    "import time\n",
    "\n",
    "from helperFunctions import *\n",
    "\n",
    "\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "#print all cell contents \n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n"
   ]
  },
  {
   "source": [
    "* For training \n",
    "    + Use 50 files ~ 200K samples\n",
    "    + The shuffle buffer will be filled with these 200K samples (Memory utilisation 25GB)\n",
    "* For Validation \n",
    "    + 5 files = 15K samples (~6%)\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Configuation\n",
    "TRAIN_FILES_FOLDER = '../data/Train'\n",
    "VAL_FILES_FOLDER = '../data/Validation'\n",
    "TEST_FILES_FOLDER = '../data/Test'\n",
    "\n",
    "TRAIN_STEPS_PER_EPOCH_MULTIPLIER = 2\n",
    "VAL_STEPS_PER_EPOCH_MULTIPLIER = 2\n",
    "\n",
    "data_config = dict(INPUT_SHAPE = (128,128,3)\n",
    "                    ,TRAIN_FILES = 50\n",
    "                    ,TRAIN_BATCH_SIZE = 512\n",
    "                    ,VAL_FILES = 5\n",
    "                    ,VAL_BATCH_SIZE = 512\n",
    "                    ,PREFETCH = 5\n",
    "                  )\n",
    "\n",
    "\n",
    "data_config.update(TRAIN_SHUFFLE_BUFFER_SIZE = samplesCount(data_config['TRAIN_FILES'],TRAIN_FILES_FOLDER))\n",
    "data_config.update(TRAIN_STEPS_PER_EPOCH = round(data_config['TRAIN_SHUFFLE_BUFFER_SIZE']/data_config['TRAIN_BATCH_SIZE'])*TRAIN_STEPS_PER_EPOCH_MULTIPLIER)\n",
    "\n",
    "data_config.update(VAL_SHUFFLE_BUFFER_SIZE = samplesCount(data_config['VAL_FILES'],VAL_FILES_FOLDER))\n",
    "data_config.update(VAL_STEPS_PER_EPOCH = round(data_config['VAL_SHUFFLE_BUFFER_SIZE']/data_config['VAL_BATCH_SIZE'])*VAL_STEPS_PER_EPOCH_MULTIPLIER)\n",
    "     \n",
    "# samplesCount(data_config['TRAIN_FILES'],TRAIN_FILES_FOLDER)\n",
    "# samplesCount(data_config['VAL_FILES'],VAL_FILES_FOLDER)\n",
    "\n",
    "\n",
    "### Model Configuration\n",
    "model_config = dict(\n",
    "      EXPERIMENT = 'CNN Baseline'\n",
    "      ,METRICS = [ keras.metrics.Precision(name='precision'),keras.metrics.Recall(name='recall'),keras.metrics.AUC(name='auc')]\n",
    "      ,LR = 1e-4\n",
    "      ,EPOCHS = 100\n",
    "      ,VAL_FREQUENCY = 1\n",
    ")\n",
    "\n",
    "### Data Loading\n",
    "train = createIODataset(data_config['TRAIN_FILES'],TRAIN_FILES_FOLDER)\n",
    "val = createIODataset(data_config['VAL_FILES'],VAL_FILES_FOLDER)\n",
    "\n",
    "train = train.shuffle(buffer_size=data_config['TRAIN_SHUFFLE_BUFFER_SIZE'],reshuffle_each_iteration=True)\n",
    "train = train.repeat(-1)\n",
    "train = train.batch(data_config['TRAIN_BATCH_SIZE'],drop_remainder=True)\n",
    "train = train.prefetch(data_config['PREFETCH'])\n",
    "\n",
    "val = val.shuffle(buffer_size=data_config['VAL_SHUFFLE_BUFFER_SIZE'],reshuffle_each_iteration=True)\n",
    "val = val.repeat(-1)\n",
    "val = val.batch(data_config['VAL_BATCH_SIZE'],drop_remainder=True)\n",
    "val = val.prefetch(data_config['PREFETCH'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "source": [
    "## 1. CNN Baseline"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nrescaling (Rescaling)        (None, 128, 128, 3)       0         \n_________________________________________________________________\nbatch_normalization (BatchNo (None, 128, 128, 3)       12        \n_________________________________________________________________\nconv2d (Conv2D)              (None, 64, 64, 8)         224       \n_________________________________________________________________\nconv2d_1 (Conv2D)            (None, 32, 32, 16)        1168      \n_________________________________________________________________\nconv2d_2 (Conv2D)            (None, 16, 16, 32)        4640      \n_________________________________________________________________\nconv2d_3 (Conv2D)            (None, 8, 8, 64)          18496     \n_________________________________________________________________\nflatten (Flatten)            (None, 4096)              0         \n_________________________________________________________________\ndense (Dense)                (None, 128)               524416    \n_________________________________________________________________\ndense_1 (Dense)              (None, 128)               16512     \n_________________________________________________________________\ndense_2 (Dense)              (None, 1)                 129       \n=================================================================\nTotal params: 565,597\nTrainable params: 565,591\nNon-trainable params: 6\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(keras.layers.Input(shape=data_config['INPUT_SHAPE']))    \n",
    "model.add(tf.keras.layers.experimental.preprocessing.Rescaling(1./255))\n",
    "model.add(tf.keras.layers.BatchNormalization())            \n",
    "\n",
    "for filters in [8,16,32,64]:\n",
    "    model.add(tf.keras.layers.Conv2D(filters = filters,kernel_size = (3,3), strides = (2,2), padding='same'\n",
    "                                        ,activation='relu',kernel_initializer='he_normal'))\n",
    "\n",
    "# model.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "model.add(tf.keras.layers.Flatten())    \n",
    "\n",
    "for units in [128,128]:\n",
    "    model.add(tf.keras.layers.Dense(units,activation='relu',kernel_initializer='he_normal'))\n",
    "\n",
    "\n",
    "model.add(tf.keras.layers.Dense(1,activation='sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(project=\"candlestick-CNN\", name = model_config['EXPERIMENT'] ,reinit= True,dir = '../data/'\n",
    "                    ,config = {**data_config,**model_config})\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=model_config['LR'])\n",
    "                        ,loss=tf.keras.losses.binary_crossentropy\n",
    "                        ,metrics=model_config['METRICS'])\n",
    "\n",
    "history = model.fit(train\n",
    "                ,epochs=model_config['EPOCHS']\n",
    "                ,steps_per_epoch=data_config['TRAIN_STEPS_PER_EPOCH']\n",
    "                ,verbose=1\n",
    "                ,validation_data=val                \n",
    "                ,validation_freq = model_config['VAL_FREQUENCY']\n",
    "                ,validation_steps = data_config['VAL_STEPS_PER_EPOCH']\n",
    "                ,callbacks=[WandbCallback()]\n",
    "                )\n",
    "  \n",
    "run.finish()\n",
    "\n",
    "# model.save('../data/saved_models/' + model_config['EXPERIMENT'] )\n"
   ]
  },
  {
   "source": [
    "![CNN-Baseline-losses](./screenshots/20201117_cnn_baseline_losses.png)\n",
    "\n",
    "## Observations\n",
    "+ Model converging much faster than the Baseline Fully Connected Dense Model but overfits\n",
    "+ Try Dropout\n",
    "    \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 2. Introduce Dropout 0.1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nrescaling (Rescaling)        (None, 128, 128, 3)       0         \n_________________________________________________________________\nbatch_normalization (BatchNo (None, 128, 128, 3)       12        \n_________________________________________________________________\nconv2d (Conv2D)              (None, 64, 64, 8)         224       \n_________________________________________________________________\nconv2d_1 (Conv2D)            (None, 32, 32, 16)        1168      \n_________________________________________________________________\nconv2d_2 (Conv2D)            (None, 16, 16, 32)        4640      \n_________________________________________________________________\nconv2d_3 (Conv2D)            (None, 8, 8, 64)          18496     \n_________________________________________________________________\ndropout (Dropout)            (None, 8, 8, 64)          0         \n_________________________________________________________________\nflatten (Flatten)            (None, 4096)              0         \n_________________________________________________________________\ndense (Dense)                (None, 128)               524416    \n_________________________________________________________________\ndense_1 (Dense)              (None, 128)               16512     \n_________________________________________________________________\ndense_2 (Dense)              (None, 1)                 129       \n=================================================================\nTotal params: 565,597\nTrainable params: 565,591\nNon-trainable params: 6\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_config.update(EXPERIMENT = 'CNN Dropout')\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(keras.layers.Input(shape=data_config['INPUT_SHAPE']))    \n",
    "model.add(tf.keras.layers.experimental.preprocessing.Rescaling(1./255))\n",
    "model.add(tf.keras.layers.BatchNormalization())            \n",
    "\n",
    "for filters in [8,16,32,64]:\n",
    "    model.add(tf.keras.layers.Conv2D(filters = filters,kernel_size = (3,3), strides = (2,2), padding='same'\n",
    "                                        ,activation='relu',kernel_initializer='he_normal'))\n",
    "\n",
    "model.add(tf.keras.layers.Dropout(0.1))\n",
    "\n",
    "model.add(tf.keras.layers.Flatten())    \n",
    "\n",
    "for units in [128,128]:\n",
    "    model.add(tf.keras.layers.Dense(units,activation='relu',kernel_initializer='he_normal'))\n",
    "    \n",
    "\n",
    "\n",
    "model.add(tf.keras.layers.Dense(1,activation='sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(project=\"candlestick-CNN\", name = model_config['EXPERIMENT'] ,reinit= True,dir = '../data/'\n",
    "                    ,config = {**data_config,**model_config})\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=model_config['LR'])\n",
    "                        ,loss=tf.keras.losses.binary_crossentropy\n",
    "                        ,metrics=model_config['METRICS'])\n",
    "\n",
    "\n",
    "history = model.fit(train\n",
    "                ,epochs=model_config['EPOCHS']\n",
    "                ,steps_per_epoch=data_config['TRAIN_STEPS_PER_EPOCH']\n",
    "                ,verbose=1\n",
    "                ,validation_data=val                \n",
    "                ,validation_freq = model_config['VAL_FREQUENCY']\n",
    "                ,validation_steps = data_config['VAL_STEPS_PER_EPOCH']\n",
    "                ,callbacks=[WandbCallback()]\n",
    "                # ,ckpt_callback]\n",
    "                )\n",
    "  \n",
    "run.finish()\n",
    "\n"
   ]
  },
  {
   "source": [
    "![CNN-Baseline-losses](./screenshots/20201117_cnn_dropout_losses.png)\n",
    "\n",
    "## Observations\n",
    "+ Converges slowly but still overfits the validation set\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 3. Try with the cleaned Train and Validation sets with Dropout\n",
    "\n",
    "* TO DO : Use original (unclean) Validation and Test"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "![CNN-Baseline-losses](./screenshots/20201117_cnn_cleaned_dataset_dropout_losses.png)\n",
    "\n",
    "## Observations\n",
    "* 3% of the outlier samples were removed but no improvement in model performance\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "##  4.Hyperparameter Tuning - LR and Dropout"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kerastuner as kt\n",
    "\n",
    "## Data Configuation\n",
    "TRAIN_FILES_FOLDER = '../data/Train_Clean'\n",
    "VAL_FILES_FOLDER = '../data/Validation_Clean'\n",
    "TEST_FILES_FOLDER = '../data/Test_Clean'\n",
    "\n",
    "TRAIN_STEPS_PER_EPOCH_MULTIPLIER = 2\n",
    "VAL_STEPS_PER_EPOCH_MULTIPLIER = 2\n",
    "\n",
    "data_config = dict(INPUT_SHAPE = (128,128,3)\n",
    "                    ,TRAIN_FILES = 50\n",
    "                    ,TRAIN_BATCH_SIZE = 512\n",
    "                    ,VAL_FILES = 5\n",
    "                    ,VAL_BATCH_SIZE = 512\n",
    "                    ,PREFETCH = 5\n",
    "                  )\n",
    "\n",
    "\n",
    "data_config.update(TRAIN_SHUFFLE_BUFFER_SIZE = 100000)\n",
    "# data_config.update(TRAIN_SHUFFLE_BUFFER_SIZE = samplesCount(data_config['TRAIN_FILES'],TRAIN_FILES_FOLDER))\n",
    "\n",
    "data_config.update(TRAIN_STEPS_PER_EPOCH = round(data_config['TRAIN_SHUFFLE_BUFFER_SIZE']/data_config['TRAIN_BATCH_SIZE'])*TRAIN_STEPS_PER_EPOCH_MULTIPLIER)\n",
    "\n",
    "data_config.update(VAL_SHUFFLE_BUFFER_SIZE = samplesCount(data_config['VAL_FILES'],VAL_FILES_FOLDER))\n",
    "data_config.update(VAL_STEPS_PER_EPOCH = round(data_config['VAL_SHUFFLE_BUFFER_SIZE']/data_config['VAL_BATCH_SIZE'])*VAL_STEPS_PER_EPOCH_MULTIPLIER)\n",
    "     \n",
    "# samplesCount(data_config['TRAIN_FILES'],TRAIN_FILES_FOLDER)\n",
    "# samplesCount(data_config['VAL_FILES'],VAL_FILES_FOLDER)\n",
    "\n",
    "\n",
    "### Model Configuration\n",
    "model_config = dict(\n",
    "      EXPERIMENT = 'CNN Baseline'\n",
    "      ,METRICS = [ keras.metrics.Precision(name='precision'),keras.metrics.Recall(name='recall'),keras.metrics.AUC(name='auc')]\n",
    "      ,LR = 1e-4\n",
    "      ,EPOCHS = 100\n",
    "      ,VAL_FREQUENCY = 1\n",
    ")\n",
    "\n",
    "### Data Loading\n",
    "train = createIODataset(data_config['TRAIN_FILES'],TRAIN_FILES_FOLDER)\n",
    "val = createIODataset(data_config['VAL_FILES'],VAL_FILES_FOLDER)\n",
    "\n",
    "train = train.shuffle(buffer_size=data_config['TRAIN_SHUFFLE_BUFFER_SIZE'],reshuffle_each_iteration=True)\n",
    "train = train.repeat(-1)\n",
    "train = train.batch(data_config['TRAIN_BATCH_SIZE'],drop_remainder=True)\n",
    "train = train.prefetch(data_config['PREFETCH'])\n",
    "\n",
    "val = val.shuffle(buffer_size=data_config['VAL_SHUFFLE_BUFFER_SIZE'],reshuffle_each_iteration=True)\n",
    "val = val.repeat(-1)\n",
    "val = val.batch(data_config['VAL_BATCH_SIZE'],drop_remainder=True)\n",
    "val = val.prefetch(data_config['PREFETCH'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def model_builder(hp):\n",
    "\n",
    "    #HPs :\n",
    "    model_config['DROPOUT'] = hp.Float('dropout', min_value = 0.1, max_value = 0.8, step = 0.05)\n",
    "    # hp_learning_rate = hp.Float('learning_rate', min_value = 1e-8, max_value = 1e-4, step = 0.1)\n",
    "    model_config['LR'] = hp.Choice('learning_rate', values = [1e-4,1e-5,1e-6,1e-7,1e-8]) \n",
    "\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(keras.layers.Input(shape=data_config['INPUT_SHAPE']))    \n",
    "    model.add(tf.keras.layers.experimental.preprocessing.Rescaling(1./255))\n",
    "    model.add(tf.keras.layers.BatchNormalization())            \n",
    "\n",
    "    for filters in [8,16,32,64]:\n",
    "        model.add(tf.keras.layers.Conv2D(filters = filters,kernel_size = (3,3), strides = (2,2), padding='same'\n",
    "                                            ,activation='relu',kernel_initializer='he_normal'))\n",
    "\n",
    "    model.add(tf.keras.layers.Dropout(model_config['DROPOUT']))\n",
    "\n",
    "    model.add(tf.keras.layers.Flatten())    \n",
    "\n",
    "    for units in [128,128]:\n",
    "        model.add(tf.keras.layers.Dense(units,activation='relu',kernel_initializer='he_normal'))\n",
    "       \n",
    "\n",
    "    model.add(tf.keras.layers.Dense(1,activation='sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=model_config['LR'])\n",
    "                        ,loss=tf.keras.losses.binary_crossentropy\n",
    "                        ,metrics=model_config['METRICS'])\n",
    "  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config.update(EXPERIMENT = 'CNN Cleaned Keras Tuner')\n",
    "\n",
    "run = wandb.init(project=\"candlestick-CNN\", name = model_config['EXPERIMENT'] \n",
    "                    ,reinit= True,dir = '../data/'\n",
    "                    ,config = {**data_config,**model_config})\n",
    "\n",
    "\n",
    "tuner = kt.Hyperband(model_builder,\n",
    "                     objective = 'val_loss', \n",
    "                     max_epochs = 30,\n",
    "                     factor = 3,\n",
    "                     hyperband_iterations = 5,\n",
    "                     directory = '../data/kerastuner',\n",
    "                     project_name = 'intro_to_kt')                       \n",
    "\n",
    "\n",
    "# class ClearTrainingOutput(tf.keras.callbacks.Callback):\n",
    "#   def on_train_end(*args, **kwargs):\n",
    "#     IPython.display.clear_output(wait = True)                     \n",
    "\n",
    "\n",
    "# Run the hyperparameter search. \n",
    "# The arguments for the search method are the same as those used for tf.keras.model.fit in addition to the callback above.\n",
    "\n",
    "# callback_ES = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=6)\n",
    "\n",
    "\n",
    "tuner.search(train\n",
    "                ,epochs=30\n",
    "                ,steps_per_epoch=data_config['TRAIN_STEPS_PER_EPOCH']\n",
    "                ,verbose=1\n",
    "                ,validation_data=val                \n",
    "                ,validation_freq = model_config['VAL_FREQUENCY']\n",
    "                ,validation_steps = data_config['VAL_STEPS_PER_EPOCH']\n",
    "                ,callbacks=[WandbCallback()]\n",
    "                # ,ClearTrainingOutput()]\n",
    "                # ,ckpt_callback]\n",
    "                )\n",
    "  \n",
    "\n",
    "# tuner.search(set_x, set_y, epochs = 2, validation_data = (test_set_x, test_set_y)\n",
    "#                 , callbacks = [ClearTrainingOutput(),callback_ES])\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials = 1)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.0001"
      ]
     },
     "metadata": {},
     "execution_count": 17
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.25000000000000006"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "import joblib\n",
    "# joblib.dump(tuner, \"../data/kerastuner/tuner.pkl\") \n",
    "tuner =  joblib.load(\"../data/kerastuner/tuner.pkl\")\n",
    "\n",
    "\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials = 1)[0]\n",
    "\n",
    "\n",
    "best_hps.get('learning_rate')\n",
    "best_hps.get('dropout')\n"
   ]
  },
  {
   "source": [
    "## 5. LR 1e-4 and Dropout 0.25"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nrescaling_1 (Rescaling)      (None, 128, 128, 3)       0         \n_________________________________________________________________\nbatch_normalization_1 (Batch (None, 128, 128, 3)       12        \n_________________________________________________________________\nconv2d_4 (Conv2D)            (None, 64, 64, 8)         224       \n_________________________________________________________________\nconv2d_5 (Conv2D)            (None, 32, 32, 16)        1168      \n_________________________________________________________________\nconv2d_6 (Conv2D)            (None, 16, 16, 32)        4640      \n_________________________________________________________________\nconv2d_7 (Conv2D)            (None, 8, 8, 64)          18496     \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 8, 8, 64)          0         \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 4096)              0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 128)               524416    \n_________________________________________________________________\ndense_4 (Dense)              (None, 128)               16512     \n_________________________________________________________________\ndense_5 (Dense)              (None, 1)                 129       \n=================================================================\nTotal params: 565,597\nTrainable params: 565,591\nNon-trainable params: 6\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## Data Configuation\n",
    "TRAIN_FILES_FOLDER = '../data/Train_Clean'\n",
    "VAL_FILES_FOLDER = '../data/Validation_Clean'\n",
    "TEST_FILES_FOLDER = '../data/Test_Clean'\n",
    "\n",
    "TRAIN_STEPS_PER_EPOCH_MULTIPLIER = 2\n",
    "VAL_STEPS_PER_EPOCH_MULTIPLIER = 2\n",
    "\n",
    "data_config = dict(INPUT_SHAPE = (128,128,3)\n",
    "                    ,TRAIN_FILES = 50\n",
    "                    ,TRAIN_BATCH_SIZE = 512\n",
    "                    ,VAL_FILES = 5\n",
    "                    ,VAL_BATCH_SIZE = 512\n",
    "                    ,PREFETCH = 5\n",
    "                  )\n",
    "\n",
    "\n",
    "data_config.update(TRAIN_SHUFFLE_BUFFER_SIZE = 100000)\n",
    "data_config.update(TRAIN_STEPS_PER_EPOCH = round(data_config['TRAIN_SHUFFLE_BUFFER_SIZE']/data_config['TRAIN_BATCH_SIZE'])*TRAIN_STEPS_PER_EPOCH_MULTIPLIER)\n",
    "\n",
    "data_config.update(VAL_SHUFFLE_BUFFER_SIZE = samplesCount(data_config['VAL_FILES'],VAL_FILES_FOLDER))\n",
    "data_config.update(VAL_STEPS_PER_EPOCH = round(data_config['VAL_SHUFFLE_BUFFER_SIZE']/data_config['VAL_BATCH_SIZE'])*VAL_STEPS_PER_EPOCH_MULTIPLIER)\n",
    "     \n",
    "# samplesCount(data_config['TRAIN_FILES'],TRAIN_FILES_FOLDER)\n",
    "# samplesCount(data_config['VAL_FILES'],VAL_FILES_FOLDER)\n",
    "\n",
    "\n",
    "### Model Configuration\n",
    "model_config = dict(\n",
    "      EXPERIMENT = 'CNN Baseline'\n",
    "      ,METRICS = [ keras.metrics.Precision(name='precision'),keras.metrics.Recall(name='recall'),keras.metrics.AUC(name='auc')]\n",
    "      ,LR = 1e-4\n",
    "      ,EPOCHS = 250\n",
    "      ,VAL_FREQUENCY = 1\n",
    ")\n",
    "\n",
    "### Data Loading\n",
    "train = createIODataset(data_config['TRAIN_FILES'],TRAIN_FILES_FOLDER)\n",
    "val = createIODataset(data_config['VAL_FILES'],VAL_FILES_FOLDER)\n",
    "\n",
    "train = train.shuffle(buffer_size=data_config['TRAIN_SHUFFLE_BUFFER_SIZE'],reshuffle_each_iteration=True)\n",
    "train = train.repeat(-1)\n",
    "train = train.batch(data_config['TRAIN_BATCH_SIZE'],drop_remainder=True)\n",
    "train = train.prefetch(data_config['PREFETCH'])\n",
    "\n",
    "val = val.shuffle(buffer_size=data_config['VAL_SHUFFLE_BUFFER_SIZE'],reshuffle_each_iteration=True)\n",
    "val = val.repeat(-1)\n",
    "val = val.batch(data_config['VAL_BATCH_SIZE'],drop_remainder=True)\n",
    "val = val.prefetch(data_config['PREFETCH'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Architecture\n",
    "\n",
    "model_config.update(EXPERIMENT = 'CNN after Keras Tuner')\n",
    "model_config.update(DROPOUT = 0.25)\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(keras.layers.Input(shape=data_config['INPUT_SHAPE']))    \n",
    "model.add(tf.keras.layers.experimental.preprocessing.Rescaling(1./255))\n",
    "model.add(tf.keras.layers.BatchNormalization())            \n",
    "\n",
    "for filters in [8,16,32,64]:\n",
    "    model.add(tf.keras.layers.Conv2D(filters = filters,kernel_size = (3,3), strides = (2,2), padding='same'\n",
    "                                        ,activation='relu',kernel_initializer='he_normal'))\n",
    "\n",
    "model.add(tf.keras.layers.Dropout(model_config['DROPOUT']))\n",
    "\n",
    "model.add(tf.keras.layers.Flatten())    \n",
    "\n",
    "for units in [128,128]:\n",
    "    model.add(tf.keras.layers.Dense(units,activation='relu',kernel_initializer='he_normal'))\n",
    "    \n",
    "\n",
    "\n",
    "model.add(tf.keras.layers.Dense(1,activation='sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(project=\"candlestick-CNN\", name = model_config['EXPERIMENT'] ,reinit= True,dir = '../data/'\n",
    "                    ,config = {**data_config,**model_config})\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=model_config['LR'])\n",
    "                        ,loss=tf.keras.losses.binary_crossentropy\n",
    "                        ,metrics=model_config['METRICS'])\n",
    "\n",
    "\n",
    "history = model.fit(train\n",
    "                ,epochs=model_config['EPOCHS']\n",
    "                ,steps_per_epoch=data_config['TRAIN_STEPS_PER_EPOCH']\n",
    "                ,verbose=1\n",
    "                ,validation_data=val                \n",
    "                ,validation_freq = model_config['VAL_FREQUENCY']\n",
    "                ,validation_steps = data_config['VAL_STEPS_PER_EPOCH']\n",
    "                ,callbacks=[WandbCallback()]\n",
    "                # ,ckpt_callback]\n",
    "                )\n",
    "  \n",
    "run.finish()\n",
    "\n"
   ]
  },
  {
   "source": [
    "![CNN-Baseline-losses](./screenshots/20201118_cnn_dropout0.5_losses.png)\n",
    "\n",
    "## Observations\n",
    "* No improvement\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}