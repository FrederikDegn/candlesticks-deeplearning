{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.6.9 64-bit",
   "display_name": "Python 3.6.9 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "4cd7ab41f5fca4b9b44701077e38c5ffd31fe66a6cab21e0214b68d958d0e462"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Time Loss Experiments - Setup\n",
    "Results are evaluated in the **032_Time-Loss-Experiments_Evaluation.ipynb**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: GeForce RTX 2060 SUPER, compute capability 7.5\n",
      "X Shape :  (43614, 128, 128, 3) Memory size is : 2044.40625 Mb Y Shape:  (43614,)\n",
      "Values, counts, Avg Performance :  [0 1 2] [23131  9960 10523] [0.53035722 0.22836704 0.24127574]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import time\n",
    "\n",
    "from functions_dataCreation import *\n",
    "from functions_modelArchitectures import *\n",
    "from class_LRFinder import *\n",
    "\n",
    "\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_policy(policy)\n",
    "\n",
    "set_x,set_y = readXYfromDisk(noOfFiles=10,folder=\"../data/Train\") #Training data\n",
    "\n",
    "set_y[set_y == 1] = 0\n",
    "set_y[set_y == 2] = 1\n",
    "\n",
    "IMG_SIZE = 128\n",
    "\n",
    "METRICS = [\n",
    "    #   keras.metrics.TruePositives(name='tp'),\n",
    "    #   keras.metrics.FalsePositives(name='fp'),\n",
    "    #   keras.metrics.TrueNegatives(name='tn'),\n",
    "    #   keras.metrics.FalseNegatives(name='fn'), \n",
    "    #   keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      keras.metrics.Precision(name='precision'),\n",
    "      keras.metrics.Recall(name='recall'),\n",
    "      keras.metrics.AUC(name='auc'),\n",
    "]\n"
   ]
  },
  {
   "source": [
    "## Experiment I\n",
    "\n",
    "The Base model has below parameters and then train three more model architecutures with a combination of Batch Normalisation and Dropout Layers\n",
    "\n",
    "* Fixed :\n",
    "    + No of CNN layers : 2\n",
    "    + Filter Sizes : 10,20\n",
    "    + Kernel size : 7x7\n",
    "    + Pool size : Max Pooling 2x2\n",
    "    + Dropout percentage : 0.1\n",
    "    + FC Dense Layers : 2 x 128 units with relu\n",
    "    + No of epochs : 20\n",
    "    + Optimisation : RMSProp\n",
    "    \n",
    "    \n",
    "* Variable\n",
    "    + No if images : 10K and 40K\n",
    "    + Learning Rates : 1e-2,1e-3,1e-4,1e-5 \n",
    "    + Batch Sizes : 64,128,256    \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "X Shape :  (43614, 128, 128, 3) Memory size is : 2044.40625 Mb Y Shape:  (43614,)\nValues, counts, Avg Performance :  [0 1 2] [23131  9960 10523] [0.53035722 0.22836704 0.24127574]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "modelBase = {}\n",
    "modelBase['name'] = 'Base'\n",
    "modelBase['inputShape'] = (IMG_SIZE,IMG_SIZE,3)\n",
    "modelBase['activation'] = 'relu'\n",
    "\n",
    "modelBase['filters'] = [10,20]\n",
    "modelBase['convLayerMultiplier'] = 1\n",
    "modelBase['kernelSize'] = (7,7)\n",
    "\n",
    "modelBase['poolingLayer'] = 'MaxPooling2D'\n",
    "modelBase['poolSize'] = (2,2)\n",
    "modelBase['padding'] = 'same'\n",
    "\n",
    "modelBase['denseLayers'] = 2\n",
    "modelBase['units'] = 128\n",
    "modelBase['activation'] = 'relu'\n",
    "\n",
    "#with Dropout and BN\n",
    "modelBase_with_BN_Dropout = modelBase.copy()\n",
    "modelBase_with_BN_Dropout['name'] = 'modelBase_with_BN_Dropout'\n",
    "modelBase_with_BN_Dropout['batchnormalization'] = True\n",
    "modelBase_with_BN_Dropout['dropout'] = 0.1\n",
    "\n",
    "#with Dropout only\n",
    "modelBase_with_Dropout = modelBase.copy()\n",
    "modelBase_with_Dropout['name'] = 'modelBase_with_Dropout'\n",
    "modelBase_with_Dropout['dropout'] = 0.1\n",
    "\n",
    "#with BN only\n",
    "modelBase_with_BN = modelBase.copy()\n",
    "modelBase_with_BN['name'] = 'modelBase_with_BN'\n",
    "modelBase_with_BN['batchnormalization'] = True\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Elapsed time  1.54\n",
      "Model_modelBase_with_Dropout_IMGs_10000_LR_0.01_BS_64\n",
      "Elapsed time  1.22\n",
      "Model_modelBase_with_Dropout_IMGs_10000_LR_0.01_BS_128\n",
      "Elapsed time  1.08\n",
      "Model_modelBase_with_Dropout_IMGs_10000_LR_0.01_BS_256\n",
      "Elapsed time  1.58\n",
      "Model_modelBase_with_Dropout_IMGs_10000_LR_0.001_BS_64\n",
      "Elapsed time  1.25\n",
      "Model_modelBase_with_Dropout_IMGs_10000_LR_0.001_BS_128\n",
      "Elapsed time  1.1\n",
      "Model_modelBase_with_Dropout_IMGs_10000_LR_0.001_BS_256\n",
      "Elapsed time  1.6\n",
      "Model_modelBase_with_Dropout_IMGs_10000_LR_0.0001_BS_64\n",
      "Elapsed time  1.25\n",
      "Model_modelBase_with_Dropout_IMGs_10000_LR_0.0001_BS_128\n",
      "Elapsed time  1.1\n",
      "Model_modelBase_with_Dropout_IMGs_10000_LR_0.0001_BS_256\n",
      "Elapsed time  1.6\n",
      "Model_modelBase_with_Dropout_IMGs_10000_LR_1e-05_BS_64\n",
      "Elapsed time  1.25\n",
      "Model_modelBase_with_Dropout_IMGs_10000_LR_1e-05_BS_128\n",
      "Elapsed time  1.13\n",
      "Model_modelBase_with_Dropout_IMGs_10000_LR_1e-05_BS_256\n",
      "Elapsed time  1.58\n",
      "Model_modelBase_with_Dropout_IMGs_40000_LR_0.01_BS_64\n",
      "Elapsed time  1.23\n",
      "Model_modelBase_with_Dropout_IMGs_40000_LR_0.01_BS_128\n",
      "Elapsed time  1.1\n",
      "Model_modelBase_with_Dropout_IMGs_40000_LR_0.01_BS_256\n",
      "Elapsed time  1.6\n",
      "Model_modelBase_with_Dropout_IMGs_40000_LR_0.001_BS_64\n",
      "Elapsed time  1.28\n",
      "Model_modelBase_with_Dropout_IMGs_40000_LR_0.001_BS_128\n",
      "Elapsed time  1.12\n",
      "Model_modelBase_with_Dropout_IMGs_40000_LR_0.001_BS_256\n",
      "Elapsed time  1.66\n",
      "Model_modelBase_with_Dropout_IMGs_40000_LR_0.0001_BS_64\n",
      "Elapsed time  1.33\n",
      "Model_modelBase_with_Dropout_IMGs_40000_LR_0.0001_BS_128\n",
      "Elapsed time  1.18\n",
      "Model_modelBase_with_Dropout_IMGs_40000_LR_0.0001_BS_256\n",
      "Elapsed time  1.6\n",
      "Model_modelBase_with_Dropout_IMGs_40000_LR_1e-05_BS_64\n",
      "Elapsed time  1.27\n",
      "Model_modelBase_with_Dropout_IMGs_40000_LR_1e-05_BS_128\n",
      "Elapsed time  1.11\n",
      "Model_modelBase_with_Dropout_IMGs_40000_LR_1e-05_BS_256\n",
      "Elapsed time  2.49\n",
      "Model_modelBase_with_BN_IMGs_10000_LR_0.01_BS_64\n",
      "Elapsed time  2.08\n",
      "Model_modelBase_with_BN_IMGs_10000_LR_0.01_BS_128\n",
      "Elapsed time  1.9\n",
      "Model_modelBase_with_BN_IMGs_10000_LR_0.01_BS_256\n",
      "Elapsed time  2.46\n",
      "Model_modelBase_with_BN_IMGs_10000_LR_0.001_BS_64\n",
      "Elapsed time  2.1\n",
      "Model_modelBase_with_BN_IMGs_10000_LR_0.001_BS_128\n",
      "Elapsed time  1.93\n",
      "Model_modelBase_with_BN_IMGs_10000_LR_0.001_BS_256\n",
      "Elapsed time  2.49\n",
      "Model_modelBase_with_BN_IMGs_10000_LR_0.0001_BS_64\n",
      "Elapsed time  2.13\n",
      "Model_modelBase_with_BN_IMGs_10000_LR_0.0001_BS_128\n",
      "Elapsed time  1.93\n",
      "Model_modelBase_with_BN_IMGs_10000_LR_0.0001_BS_256\n",
      "Elapsed time  2.52\n",
      "Model_modelBase_with_BN_IMGs_10000_LR_1e-05_BS_64\n",
      "Elapsed time  2.11\n",
      "Model_modelBase_with_BN_IMGs_10000_LR_1e-05_BS_128\n",
      "Elapsed time  1.92\n",
      "Model_modelBase_with_BN_IMGs_10000_LR_1e-05_BS_256\n",
      "Elapsed time  2.47\n",
      "Model_modelBase_with_BN_IMGs_40000_LR_0.01_BS_64\n",
      "Elapsed time  2.09\n",
      "Model_modelBase_with_BN_IMGs_40000_LR_0.01_BS_128\n",
      "Elapsed time  1.91\n",
      "Model_modelBase_with_BN_IMGs_40000_LR_0.01_BS_256\n",
      "Elapsed time  2.51\n",
      "Model_modelBase_with_BN_IMGs_40000_LR_0.001_BS_64\n",
      "Elapsed time  2.09\n",
      "Model_modelBase_with_BN_IMGs_40000_LR_0.001_BS_128\n",
      "Elapsed time  1.92\n",
      "Model_modelBase_with_BN_IMGs_40000_LR_0.001_BS_256\n",
      "Elapsed time  2.51\n",
      "Model_modelBase_with_BN_IMGs_40000_LR_0.0001_BS_64\n",
      "Elapsed time  2.11\n",
      "Model_modelBase_with_BN_IMGs_40000_LR_0.0001_BS_128\n",
      "Elapsed time  1.92\n",
      "Model_modelBase_with_BN_IMGs_40000_LR_0.0001_BS_256\n",
      "Elapsed time  2.51\n",
      "Model_modelBase_with_BN_IMGs_40000_LR_1e-05_BS_64\n",
      "Elapsed time  2.09\n",
      "Model_modelBase_with_BN_IMGs_40000_LR_1e-05_BS_128\n",
      "Elapsed time  1.92\n",
      "Model_modelBase_with_BN_IMGs_40000_LR_1e-05_BS_256\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "    epoch      loss  precision    recall       auc  elapsed  \\\n",
       "0       0  2.384405   0.231844  0.033933  0.497361     1.54   \n",
       "1       1  0.684890   0.000000  0.000000  0.500000     1.54   \n",
       "2       2  0.684890   0.000000  0.000000  0.500000     1.54   \n",
       "3       3  0.684890   0.000000  0.000000  0.500000     1.54   \n",
       "4       4  0.684890   0.000000  0.000000  0.500000     1.54   \n",
       "..    ...       ...        ...       ...       ...      ...   \n",
       "15     15  0.433874   0.640374  0.979150  0.981762     1.92   \n",
       "16     16  0.422056   0.664367  0.984056  0.985370     1.92   \n",
       "17     17  0.407500   0.683645  0.987735  0.990294     1.92   \n",
       "18     18  0.392541   0.713069  0.992641  0.993168     1.92   \n",
       "19     19  0.381549   0.725291  0.993050  0.995274     1.92   \n",
       "\n",
       "                                               params  \n",
       "0   Model_modelBase_with_Dropout_IMGs_10000_LR_0.0...  \n",
       "1   Model_modelBase_with_Dropout_IMGs_10000_LR_0.0...  \n",
       "2   Model_modelBase_with_Dropout_IMGs_10000_LR_0.0...  \n",
       "3   Model_modelBase_with_Dropout_IMGs_10000_LR_0.0...  \n",
       "4   Model_modelBase_with_Dropout_IMGs_10000_LR_0.0...  \n",
       "..                                                ...  \n",
       "15  Model_modelBase_with_BN_IMGs_40000_LR_1e-05_BS...  \n",
       "16  Model_modelBase_with_BN_IMGs_40000_LR_1e-05_BS...  \n",
       "17  Model_modelBase_with_BN_IMGs_40000_LR_1e-05_BS...  \n",
       "18  Model_modelBase_with_BN_IMGs_40000_LR_1e-05_BS...  \n",
       "19  Model_modelBase_with_BN_IMGs_40000_LR_1e-05_BS...  \n",
       "\n",
       "[960 rows x 7 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>epoch</th>\n      <th>loss</th>\n      <th>precision</th>\n      <th>recall</th>\n      <th>auc</th>\n      <th>elapsed</th>\n      <th>params</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>2.384405</td>\n      <td>0.231844</td>\n      <td>0.033933</td>\n      <td>0.497361</td>\n      <td>1.54</td>\n      <td>Model_modelBase_with_Dropout_IMGs_10000_LR_0.0...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0.684890</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.500000</td>\n      <td>1.54</td>\n      <td>Model_modelBase_with_Dropout_IMGs_10000_LR_0.0...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>0.684890</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.500000</td>\n      <td>1.54</td>\n      <td>Model_modelBase_with_Dropout_IMGs_10000_LR_0.0...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>0.684890</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.500000</td>\n      <td>1.54</td>\n      <td>Model_modelBase_with_Dropout_IMGs_10000_LR_0.0...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>0.684890</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.500000</td>\n      <td>1.54</td>\n      <td>Model_modelBase_with_Dropout_IMGs_10000_LR_0.0...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>15</td>\n      <td>0.433874</td>\n      <td>0.640374</td>\n      <td>0.979150</td>\n      <td>0.981762</td>\n      <td>1.92</td>\n      <td>Model_modelBase_with_BN_IMGs_40000_LR_1e-05_BS...</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>16</td>\n      <td>0.422056</td>\n      <td>0.664367</td>\n      <td>0.984056</td>\n      <td>0.985370</td>\n      <td>1.92</td>\n      <td>Model_modelBase_with_BN_IMGs_40000_LR_1e-05_BS...</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>17</td>\n      <td>0.407500</td>\n      <td>0.683645</td>\n      <td>0.987735</td>\n      <td>0.990294</td>\n      <td>1.92</td>\n      <td>Model_modelBase_with_BN_IMGs_40000_LR_1e-05_BS...</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>18</td>\n      <td>0.392541</td>\n      <td>0.713069</td>\n      <td>0.992641</td>\n      <td>0.993168</td>\n      <td>1.92</td>\n      <td>Model_modelBase_with_BN_IMGs_40000_LR_1e-05_BS...</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>19</td>\n      <td>0.381549</td>\n      <td>0.725291</td>\n      <td>0.993050</td>\n      <td>0.995274</td>\n      <td>1.92</td>\n      <td>Model_modelBase_with_BN_IMGs_40000_LR_1e-05_BS...</td>\n    </tr>\n  </tbody>\n</table>\n<p>960 rows × 7 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "\n",
    "for mod in [modelBase, modelBase_with_BN_Dropout, modelBase_with_Dropout,modelBase_with_BN]:\n",
    "        for images in [10000,40000]:\n",
    "                for lr in [1e-2,1e-3,1e-4,1e-5]:\n",
    "                        for bs in [64,128,256]:\n",
    "                        \n",
    "                                start_time = time.time()\n",
    "\n",
    "                                set_x = set_x[:images]\n",
    "                                set_y = set_y[:images]\n",
    "                                \n",
    "                                model = createCNN(mod)\n",
    "                                \n",
    "                                model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=lr)\n",
    "                                        ,loss=tf.keras.losses.binary_crossentropy\n",
    "                                        ,metrics=[METRICS])\n",
    "\n",
    "                                history = model.fit(set_x,set_y\n",
    "                                                ,batch_size = bs\n",
    "                                                ,epochs=20\n",
    "                                                ,verbose=0                                                \n",
    "                                                )\n",
    "\n",
    "                                temp = pd.DataFrame(history.history).rename_axis(\"epoch\").reset_index()\n",
    "\n",
    "                                print(\"Elapsed time \", round((time.time() - start_time)/60,2))\n",
    "                                temp['elapsed'] = round((time.time() - start_time)/60,2)\n",
    "                                temp['params'] = \"Model_\" + model.name + \"_IMGs_\" + str(images) + \"_LR_\" + str(lr) + \"_BS_\" + str(bs)\n",
    "\n",
    "                                print(\"Model_\" + model.name + \"_IMGs_\" + str(images) + \"_LR_\" + str(lr) + \"_BS_\" + str(bs))\n",
    "\n",
    "                                df = df.append(temp)\n",
    "\n",
    "df.to_pickle(\"./Time-Loss-Data_BNandDropouts.pkl\")\n"
   ]
  },
  {
   "source": [
    "## Experiment II\n",
    "\n",
    "The LR 1e-4, BS 128 model with 40K images was selected to evaluate the effect of different CNN Layer parameters\n",
    "\n",
    "* Fixed :\n",
    "    + No of Images : 40K\n",
    "    + Batch Sizes : 128       \n",
    "    + No of epochs : 40\n",
    "    + Optimisation : RMSProp\n",
    "    + Learning Rates : 1e-4\n",
    "    + Dropout layers with percentage : 0.05\n",
    "    + FC Dense Layers : 2 x 128 units with relu\n",
    "    + Batch Normalisation\n",
    "    \n",
    "* Variable\n",
    "    + No of layers : Deep, Deeper, Deepest\n",
    "    + Filter Sizes : Variable\n",
    "    + Kernel size : (3x3,7x7,15x15)\n",
    "    + Pool size : Max Pooling (2x2,6x6,10x10)   \n",
    "  \n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Elapsed time 10.83deep_Kernels_(3, 3)_poolSize_(2, 2)\n",
      "Elapsed time 8.69deep_Kernels_(3, 3)_poolSize_(6, 6)\n",
      "Elapsed time 8.53deep_Kernels_(3, 3)_poolSize_(10, 10)\n",
      "Elapsed time 18.18deep_Kernels_(7, 7)_poolSize_(2, 2)\n",
      "Elapsed time 13.61deep_Kernels_(7, 7)_poolSize_(6, 6)\n",
      "Elapsed time 13.21deep_Kernels_(7, 7)_poolSize_(10, 10)\n",
      "Elapsed time 60.01deep_Kernels_(15, 15)_poolSize_(2, 2)\n",
      "Elapsed time 36.74deep_Kernels_(15, 15)_poolSize_(6, 6)\n",
      "Elapsed time 35.35deep_Kernels_(15, 15)_poolSize_(10, 10)\n",
      "Elapsed time 11.51deeper_Kernels_(3, 3)_poolSize_(2, 2)\n",
      "Elapsed time 9.69deeper_Kernels_(3, 3)_poolSize_(6, 6)\n",
      "Elapsed time 9.46deeper_Kernels_(3, 3)_poolSize_(10, 10)\n",
      "Elapsed time 19.16deeper_Kernels_(7, 7)_poolSize_(2, 2)\n",
      "Elapsed time 14.46deeper_Kernels_(7, 7)_poolSize_(6, 6)\n",
      "Elapsed time 14.32deeper_Kernels_(7, 7)_poolSize_(10, 10)\n",
      "Elapsed time 62.34deeper_Kernels_(15, 15)_poolSize_(2, 2)\n",
      "Elapsed time 38.76deeper_Kernels_(15, 15)_poolSize_(6, 6)\n",
      "Elapsed time 37.58deeper_Kernels_(15, 15)_poolSize_(10, 10)\n",
      "Elapsed time 13.17deepest_Kernels_(3, 3)_poolSize_(2, 2)\n",
      "Elapsed time 11.01deepest_Kernels_(3, 3)_poolSize_(6, 6)\n",
      "Elapsed time 11.1deepest_Kernels_(3, 3)_poolSize_(10, 10)\n",
      "Elapsed time 21.55deepest_Kernels_(7, 7)_poolSize_(2, 2)\n",
      "Elapsed time 16.63deepest_Kernels_(7, 7)_poolSize_(6, 6)\n",
      "Elapsed time 16.44deepest_Kernels_(7, 7)_poolSize_(10, 10)\n",
      "Elapsed time 69.4deepest_Kernels_(15, 15)_poolSize_(2, 2)\n",
      "Elapsed time 46.47deepest_Kernels_(15, 15)_poolSize_(6, 6)\n",
      "Elapsed time 45.21deepest_Kernels_(15, 15)_poolSize_(10, 10)\n"
     ]
    }
   ],
   "source": [
    "modelBase = {}\n",
    "modelBase['name'] = 'Base'\n",
    "modelBase['inputShape'] = (IMG_SIZE,IMG_SIZE,3)\n",
    "modelBase['activation'] = 'relu'\n",
    "\n",
    "modelBase['convLayerMultiplier'] = 1\n",
    "\n",
    "modelBase['poolingLayer'] = 'MaxPooling2D'\n",
    "modelBase['poolSize'] = (2,2)\n",
    "modelBase['padding'] = 'same'\n",
    "\n",
    "modelBase['denseLayers'] = 2\n",
    "modelBase['units'] = 128\n",
    "modelBase['activation'] = 'relu'\n",
    "\n",
    "#with Dropout and BN\n",
    "modelBase_with_BN_Dropout = modelBase.copy()\n",
    "modelBase_with_BN_Dropout['name'] = 'modelBase_with_BN_Dropout'\n",
    "modelBase_with_BN_Dropout['batchnormalization'] = True\n",
    "modelBase_with_BN_Dropout['dropout'] = 0.05\n",
    "\n",
    "filtersDict = {'deep' : [10,15,20,25,30]\n",
    "                        ,'deeper':[10,15,20,25,30,35,40,45,50,55,60]\n",
    "                        ,'deepest':[10,15,20,25,30,35,40,45,50,55,60,65,70,75,80,85,90,95,100]}\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for fskey,fsvalue in filtersDict.items():\n",
    "        for ks in [(3,3),(7,7),(15,15)]:\n",
    "            for ps in [(2,2),(6,6),(10,10)]:\n",
    "\n",
    "                start_time = time.time()\n",
    "\n",
    "                set_x = set_x[:40000]\n",
    "                set_y = set_y[:40000]\n",
    "                \n",
    "                modelBase_with_BN_Dropout['filters'] = fsvalue\n",
    "                modelBase_with_BN_Dropout['kernelSize'] = ks\n",
    "                modelBase_with_BN_Dropout['poolSize'] = ps\n",
    "\n",
    "                model = createCNN(modelBase_with_BN_Dropout)\n",
    "                \n",
    "                model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=1e-4)\n",
    "                        ,loss=tf.keras.losses.binary_crossentropy\n",
    "                        ,metrics=[METRICS])\n",
    "\n",
    "                history = model.fit(set_x,set_y\n",
    "                                ,batch_size = 128\n",
    "                                ,epochs=40\n",
    "                                ,verbose=0                                                \n",
    "                                )\n",
    "\n",
    "                temp = pd.DataFrame(history.history).rename_axis(\"epoch\")\n",
    "\n",
    "                \n",
    "                temp['elapsed'] = round((time.time() - start_time)/60,2)\n",
    "                temp['params'] = fskey + \"_Kernels_\" + str(ks) + \"_poolSize_\" + str(ps)\n",
    "\n",
    "                print(\"Elapsed time \" + str(round((time.time() - start_time)/60,2)) + fskey + \n",
    "                                \"_Kernels_\" + str(ks) + \"_poolSize_\" + str(ps)) \n",
    "\n",
    "                df = df.append(temp)\n",
    "\n",
    "\n",
    "\n",
    "df.to_pickle('./Time-Loss-Data_FilterKernelPool.pkl')\n"
   ]
  },
  {
   "source": [
    "\n",
    "## Large kernel size means more training time, so lets keep it small and change depth and poolsize to see which ones gives low loss\n",
    "\n",
    "* Depth of the network has no impact on the training time\n",
    "* Pool Size also has not impact on training time\n",
    "\n",
    "Create Deepest network\n",
    "Keep Kernel size as 3,3\n",
    "Try different pool sizes to get min loss\n",
    "\n",
    "## Experiment III\n",
    "\n",
    "The LR 1e-4, BS 128 model with 40K images was selected to evaluate the effect of different CNN Layer parameters\n",
    "\n",
    "* Fixed :\n",
    "    + No of Images : 40K\n",
    "    + Batch Sizes : 128       \n",
    "    + No of epochs : 60 (**Increased**)\n",
    "    + Optimisation : RMSProp\n",
    "    + Learning Rates : 1e-4\n",
    "    + Dropout layers with percentage : 0.05\n",
    "    + FC Dense Layers : 2 x 128 units with relu\n",
    "    + Batch Normalisation\n",
    "    + Kernel size : (3x3)\n",
    "    \n",
    "    \n",
    "* Variable\n",
    "    + No of layers : Deep, Deeper, Deepest\n",
    "    + Filter Sizes : Variable    \n",
    "    + Pool size : Max Pooling (2x2,6x6,10x10)   \n",
    "  \n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Elapsed time 15.8deep_poolSize_(2, 2)\n",
      "Elapsed time 13.12deep_poolSize_(6, 6)\n",
      "Elapsed time 13.06deep_poolSize_(10, 10)\n",
      "Elapsed time 17.29deeper_poolSize_(2, 2)\n",
      "Elapsed time 14.75deeper_poolSize_(6, 6)\n",
      "Elapsed time 14.31deeper_poolSize_(10, 10)\n",
      "Elapsed time 19.8deepest_poolSize_(2, 2)\n",
      "Elapsed time 16.41deepest_poolSize_(6, 6)\n",
      "Elapsed time 16.46deepest_poolSize_(10, 10)\n"
     ]
    }
   ],
   "source": [
    "modelBase = {}\n",
    "modelBase['name'] = 'Base'\n",
    "modelBase['inputShape'] = (IMG_SIZE,IMG_SIZE,3)\n",
    "modelBase['activation'] = 'relu'\n",
    "\n",
    "modelBase['convLayerMultiplier'] = 1\n",
    "\n",
    "modelBase['poolingLayer'] = 'MaxPooling2D'\n",
    "modelBase['poolSize'] = (2,2)\n",
    "modelBase['padding'] = 'same'\n",
    "\n",
    "modelBase['denseLayers'] = 2\n",
    "modelBase['units'] = 128\n",
    "modelBase['activation'] = 'relu'\n",
    "\n",
    "#with Dropout and BN\n",
    "modelBase_with_BN_Dropout = modelBase.copy()\n",
    "modelBase_with_BN_Dropout['name'] = 'modelBase_with_BN_Dropout'\n",
    "modelBase_with_BN_Dropout['batchnormalization'] = True\n",
    "modelBase_with_BN_Dropout['dropout'] = 0.05\n",
    "\n",
    "modelBase_with_BN_Dropout['kernelSize'] = (3,3)\n",
    "\n",
    "filtersDict = {'deep' : [10,15,20,25,30]\n",
    "                        ,'deeper':[10,15,20,25,30,35,40,45,50,55,60]\n",
    "                        ,'deepest':[10,15,20,25,30,35,40,45,50,55,60,65,70,75,80,85,90,95,100]}\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for fskey,fsvalue in filtersDict.items():        \n",
    "        for ps in [(2,2),(6,6),(10,10)]:\n",
    "\n",
    "            start_time = time.time()\n",
    "\n",
    "            set_x = set_x[:40000]\n",
    "            set_y = set_y[:40000]\n",
    "            \n",
    "            modelBase_with_BN_Dropout['filters'] = fsvalue                \n",
    "            modelBase_with_BN_Dropout['poolSize'] = ps\n",
    "\n",
    "            model = createCNN(modelBase_with_BN_Dropout)\n",
    "            \n",
    "            model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=1e-4)\n",
    "                    ,loss=tf.keras.losses.binary_crossentropy\n",
    "                    ,metrics=[METRICS])\n",
    "\n",
    "            history = model.fit(set_x,set_y\n",
    "                            ,batch_size = 128\n",
    "                            ,epochs=60\n",
    "                            ,verbose=0                                                \n",
    "                            )\n",
    "\n",
    "            temp = pd.DataFrame(history.history).rename_axis(\"epoch\")\n",
    "\n",
    "            \n",
    "            temp['elapsed'] = round((time.time() - start_time)/60,2)\n",
    "            temp['params'] = fskey + \"_poolSize_\" + str(ps)\n",
    "\n",
    "            print(\"Elapsed time \" + str(round((time.time() - start_time)/60,2)) + fskey + \"_poolSize_\" + str(ps)) \n",
    "\n",
    "            df = df.append(temp)\n",
    "            df.to_pickle('./Time-Loss-Data_LayersPool.pkl')\n"
   ]
  },
  {
   "source": [
    "## Experiment IV\n",
    "\n",
    "Same as above but stop training when loss reaches 0.45\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class haltCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if(logs.get('loss') <= 0.45):\n",
    "            print(\"\\nReached 0.45 loss value so cancelling training!\\n\")\n",
    "            self.model.stop_training = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      "\n",
      "Reached 0.45 loss value so cancelling training!\n",
      "\n",
      "\n",
      "\n",
      "Elapsed time 21.68deep_poolSize_(2, 2)\n",
      "Elapsed time 215.77deep_poolSize_(6, 6)\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-5ecd35100fdc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m                             \u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                             \u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m                             \u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhaltCallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m                             )\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "modelBase = {}\n",
    "modelBase['name'] = 'Base'\n",
    "modelBase['inputShape'] = (IMG_SIZE,IMG_SIZE,3)\n",
    "modelBase['activation'] = 'relu'\n",
    "\n",
    "modelBase['convLayerMultiplier'] = 1\n",
    "\n",
    "modelBase['poolingLayer'] = 'MaxPooling2D'\n",
    "modelBase['poolSize'] = (2,2)\n",
    "modelBase['padding'] = 'same'\n",
    "\n",
    "modelBase['denseLayers'] = 2\n",
    "modelBase['units'] = 128\n",
    "modelBase['activation'] = 'relu'\n",
    "\n",
    "#with Dropout and BN\n",
    "modelBase_with_BN_Dropout = modelBase.copy()\n",
    "modelBase_with_BN_Dropout['name'] = 'modelBase_with_BN_Dropout'\n",
    "modelBase_with_BN_Dropout['batchnormalization'] = True\n",
    "modelBase_with_BN_Dropout['dropout'] = 0.05\n",
    "\n",
    "modelBase_with_BN_Dropout['kernelSize'] = (3,3)\n",
    "\n",
    "filtersDict = {'deep' : [10,15,20,25,30]\n",
    "                        ,'deeper':[10,15,20,25,30,35,40,45,50,55,60]\n",
    "                        ,'deepest':[10,15,20,25,30,35,40,45,50,55,60,65,70,75,80,85,90,95,100]}\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for fskey,fsvalue in filtersDict.items():        \n",
    "        for ps in [(2,2),(6,6),(10,10)]:\n",
    "\n",
    "            start_time = time.time()\n",
    "\n",
    "            set_x = set_x[:40000]\n",
    "            set_y = set_y[:40000]\n",
    "            \n",
    "            modelBase_with_BN_Dropout['filters'] = fsvalue                \n",
    "            modelBase_with_BN_Dropout['poolSize'] = ps\n",
    "\n",
    "            model = createCNN(modelBase_with_BN_Dropout)\n",
    "            \n",
    "            model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=1e-4)\n",
    "                    ,loss=tf.keras.losses.binary_crossentropy\n",
    "                    ,metrics=[METRICS])\n",
    "\n",
    "            history = model.fit(set_x,set_y\n",
    "                            ,batch_size = 128\n",
    "                            ,epochs=1000\n",
    "                            ,verbose=0                   \n",
    "                            ,callbacks=[haltCallback()]                             \n",
    "                            )\n",
    "\n",
    "            temp = pd.DataFrame(history.history).rename_axis(\"epoch\")\n",
    "\n",
    "            \n",
    "            temp['elapsed'] = round((time.time() - start_time)/60,2)\n",
    "            temp['params'] = fskey + \"_poolSize_\" + str(ps)\n",
    "\n",
    "            print(\"Elapsed time \" + str(round((time.time() - start_time)/60,2)) + fskey + \"_poolSize_\" + str(ps)) \n",
    "\n",
    "            df = df.append(temp)\n",
    "            df.to_pickle('./Time-Loss-Data_LayersPool_Stop-training-at-loss0.5.pkl')\n"
   ]
  },
  {
   "source": [
    "## Experiment V: NOT DONE\n",
    "## Above models not learning fast enough\n",
    "\n",
    "* Try the above setup with lower and higher learning rates. \n",
    "* Try 60 epochs unless loss drops to 0.45\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Experiment VI : NOT DONE\n",
    " \n",
    "### For the deeper network, keep everything fixed and get the best learning rate\n",
    "* Also reduce dropout and remove BN ( as BN tends to take time)\n",
    "* try with 10K and 40kimages\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}