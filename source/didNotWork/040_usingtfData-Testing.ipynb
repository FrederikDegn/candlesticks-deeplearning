{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.6.9 64-bit",
   "display_name": "Python 3.6.9 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "4cd7ab41f5fca4b9b44701077e38c5ffd31fe66a6cab21e0214b68d958d0e462"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "#import tensorflow_io as tfio\n",
    "\n",
    "import time\n",
    "\n",
    "from functions_dataCreation import *\n",
    "from functions_modelArchitectures import *\n",
    "from class_LRFinder import *\n",
    "\n",
    "\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "IMG_SIZE = 128\n",
    "\n",
    "METRICS = [\n",
    "    #   keras.metrics.TruePositives(name='tp'),\n",
    "    #   keras.metrics.FalsePositives(name='fp'),\n",
    "    #   keras.metrics.TrueNegatives(name='tn'),\n",
    "    #   keras.metrics.FalseNegatives(name='fn'), \n",
    "    #   keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      keras.metrics.Precision(name='precision'),\n",
    "      keras.metrics.Recall(name='recall'),\n",
    "      keras.metrics.AUC(name='auc'),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = createIODataset(140,'../data/Train')\n",
    "test = createIODataset(4,'../data/Test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Shuffle the elements of the dataset.\n",
    "# train = train.shuffle(buffer_size=10240,reshuffle_each_iteration=True)\n",
    "\n",
    "# # By default image data is uint8, so convert to float32 using map().\n",
    "# # my_train = my_train.map(lambda x, y: (tf.image.convert_image_dtype(x, tf.float32), y))\n",
    "\n",
    "# # prepare batches the data just like any other tf.data.Dataset\n",
    "# train = train.batch(256,drop_remainder=True).prefetch(10)\n",
    "# # train = train.repeat()\n",
    "\n",
    "# test = test.shuffle(buffer_size=2048,reshuffle_each_iteration=True).batch(256,drop_remainder=True).prefetch(10)\n",
    "# # test = test.repeat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.repeat(-1)\n",
    "# train = train.shuffle(buffer_size=10240*20,reshuffle_each_iteration=True)\n",
    "# train = train.repeat(5)\n",
    "train = train.batch(256,drop_remainder=True)\n",
    "train = train.prefetch(10)\n",
    "\n",
    "test = test.repeat(-1)\n",
    "# test = test.shuffle(buffer_size=10240,reshuffle_each_iteration=True)\n",
    "# train = train.repeat(5)\n",
    "test = test.batch(1024,drop_remainder=True)\n",
    "test = test.prefetch(10)\n",
    "\n"
   ]
  },
  {
   "source": [
    "## Faster LR\n",
    " \n",
    "* Train a CNN for 10000 epochs with LR of 1e-4\n",
    "* Changed glorot_normal for relu activation\n",
    "* Added BN for first image layer\n",
    "\n",
    "For Batch Size of 256, to complete 1 epoch the model take 2114 steps. So total dataset size is approx 500K images. Time taken per epoch is 198s = 3.5mins\n",
    "\n",
    "Lets do 200 steps per epoch i.e 51200 (samples) ie 10% of total per epoch\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Failed to query for notebook name, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mamitagni\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.9 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                Tracking run with wandb version 0.10.8<br/>\n                Syncing run <strong style=\"color:#cdcd00\">Deeper tfData 1e-6 100steps per epoch shuffle size 200K Deeper CNN NO Shuffle</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://wandb.ai/amitagni/candlestick-CNN\" target=\"_blank\">https://wandb.ai/amitagni/candlestick-CNN</a><br/>\n                Run page: <a href=\"https://wandb.ai/amitagni/candlestick-CNN/runs/vksvhz3c\" target=\"_blank\">https://wandb.ai/amitagni/candlestick-CNN/runs/vksvhz3c</a><br/>\n                Run data is saved locally in <code>wandb/run-20201109_113426-vksvhz3c</code><br/><br/>\n            "
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "\n",
      "Epoch 00500: saving model to ../data/callbacks/\n",
      "\n",
      "Epoch 01000: saving model to ../data/callbacks/\n",
      "\n",
      "Epoch 01500: saving model to ../data/callbacks/\n",
      "\n",
      "Epoch 02000: saving model to ../data/callbacks/\n",
      "\n",
      "Epoch 02500: saving model to ../data/callbacks/\n",
      "\n",
      "Epoch 03000: saving model to ../data/callbacks/\n",
      "\n",
      "Epoch 03500: saving model to ../data/callbacks/\n",
      "\n",
      "Epoch 04000: saving model to ../data/callbacks/\n",
      "\n",
      "Epoch 04500: saving model to ../data/callbacks/\n",
      "\n",
      "Epoch 05000: saving model to ../data/callbacks/\n"
     ]
    }
   ],
   "source": [
    "\n",
    "wandb.init(project=\"candlestick-CNN\", name = 'Deeper tfData 1e-7 100steps per epoch shuffle size 200K Deeper CNN NO Shuffle' )\n",
    "\n",
    "modelBase = {}\n",
    "modelBase['name'] = 'Base'\n",
    "modelBase['inputShape'] = (IMG_SIZE,IMG_SIZE,3)\n",
    "modelBase['activation'] = 'relu'\n",
    "\n",
    "modelBase['convLayerMultiplier'] = 1\n",
    "\n",
    "modelBase['poolingLayer'] = 'MaxPooling2D'\n",
    "modelBase['padding'] = 'same'\n",
    "\n",
    "modelBase['denseLayers'] = 2\n",
    "modelBase['units'] = 128\n",
    "modelBase['activation'] = 'relu'\n",
    "\n",
    "#with Dropout and BN\n",
    "modelBase_with_Dropout = modelBase.copy()\n",
    "modelBase_with_Dropout['name'] = 'modelBase_with_Dropout'\n",
    "modelBase_with_Dropout['batchnormalization'] = False\n",
    "# modelBase_with_Dropout['dropout'] = 0.00001\n",
    "\n",
    "modelBase_with_Dropout['kernelSize'] = (3,3)\n",
    "modelBase_with_Dropout['filters'] = [10,15,20,25,30,35,40,45,50,55,60]\n",
    "modelBase_with_Dropout['poolSize'] = (6,6)\n",
    "\n",
    "lr = 1e-7\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "checkpoint_path = '../data/callbacks/'\n",
    "\n",
    "# Create a callback that saves the model's weights every 5 epochs\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path, \n",
    "    verbose=1, \n",
    "    save_weights_only=True,\n",
    "    period=500)   \n",
    "\n",
    "model = createCNN(modelBase_with_Dropout)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=lr)\n",
    "        ,loss=tf.keras.losses.binary_crossentropy\n",
    "        ,metrics=[METRICS])\n",
    "\n",
    "history = model.fit(train\n",
    "                #,batch_size = 128\n",
    "                ,epochs=5000\n",
    "                ,steps_per_epoch=100\n",
    "                ,verbose=0\n",
    "                ,validation_data=test                \n",
    "                ,validation_freq = 100\n",
    "                ,validation_steps = 10\n",
    "                ,callbacks=[WandbCallback(),cp_callback]\n",
    "                )\n",
    "\n",
    "# temp = pd.DataFrame(history.history).rename_axis(\"epoch\")\n",
    "# temp['elapsed'] = round((time.time() - start_time)/60,2)\n",
    "# var_params = \"Deeper_\" + \"_LR_\"  + str(lr)\n",
    "# temp['params'] = var_params\n",
    "\n",
    "# print(\"Elapsed time \" + str(round((time.time() - start_time)/60,2)) + var_params)\n",
    "\n",
    "# model.save('../data/savedmodels') \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "614400"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "1024*100*6"
   ]
  }
 ]
}