{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.6.9 64-bit",
   "display_name": "Python 3.6.9 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "4cd7ab41f5fca4b9b44701077e38c5ffd31fe66a6cab21e0214b68d958d0e462"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "### Package Setups\n",
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "from tensorflow import keras\n",
    "import time\n",
    "\n",
    "from functions_dataCreation import *\n",
    "\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "### Data Loading\n",
    "train = createIODataset(50,'../data/Train')\n",
    "test = createIODataset(4,'../data/Test')\n",
    "\n",
    "train = train.repeat(-1)\n",
    "train = train.shuffle(buffer_size=10240,reshuffle_each_iteration=True)\n",
    "train = train.batch(256,drop_remainder=True)\n",
    "train = train.prefetch(4)\n",
    "\n",
    "test = test.repeat(-1)\n",
    "test = test.shuffle(buffer_size=10240,reshuffle_each_iteration=True)\n",
    "test = test.batch(256,drop_remainder=True)\n",
    "test = test.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MobileNetV2 expects pixel vaues in [-1,1], but at this point, the pixel values in your images are in [0-255]. \n",
    "\n",
    "preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the base model from the pre-trained model MobileNet V2\n",
    "IMG_SIZE = 128\n",
    "base_model = tf.keras.applications.MobileNetV2(input_shape=(IMG_SIZE,IMG_SIZE,3),include_top=False,weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"mobilenetv2_1.00_128\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_1 (InputLayer)            [(None, 128, 128, 3) 0                                            \n__________________________________________________________________________________________________\nConv1_pad (ZeroPadding2D)       (None, 129, 129, 3)  0           input_1[0][0]                    \n__________________________________________________________________________________________________\nConv1 (Conv2D)                  (None, 64, 64, 32)   864         Conv1_pad[0][0]                  \n__________________________________________________________________________________________________\nbn_Conv1 (BatchNormalization)   (None, 64, 64, 32)   128         Conv1[0][0]                      \n__________________________________________________________________________________________________\nConv1_relu (ReLU)               (None, 64, 64, 32)   0           bn_Conv1[0][0]                   \n__________________________________________________________________________________________________\nexpanded_conv_depthwise (Depthw (None, 64, 64, 32)   288         Conv1_relu[0][0]                 \n__________________________________________________________________________________________________\nexpanded_conv_depthwise_BN (Bat (None, 64, 64, 32)   128         expanded_conv_depthwise[0][0]    \n__________________________________________________________________________________________________\nexpanded_conv_depthwise_relu (R (None, 64, 64, 32)   0           expanded_conv_depthwise_BN[0][0] \n__________________________________________________________________________________________________\nexpanded_conv_project (Conv2D)  (None, 64, 64, 16)   512         expanded_conv_depthwise_relu[0][0\n__________________________________________________________________________________________________\nexpanded_conv_project_BN (Batch (None, 64, 64, 16)   64          expanded_conv_project[0][0]      \n__________________________________________________________________________________________________\nblock_1_expand (Conv2D)         (None, 64, 64, 96)   1536        expanded_conv_project_BN[0][0]   \n__________________________________________________________________________________________________\nblock_1_expand_BN (BatchNormali (None, 64, 64, 96)   384         block_1_expand[0][0]             \n__________________________________________________________________________________________________\nblock_1_expand_relu (ReLU)      (None, 64, 64, 96)   0           block_1_expand_BN[0][0]          \n__________________________________________________________________________________________________\nblock_1_pad (ZeroPadding2D)     (None, 65, 65, 96)   0           block_1_expand_relu[0][0]        \n__________________________________________________________________________________________________\nblock_1_depthwise (DepthwiseCon (None, 32, 32, 96)   864         block_1_pad[0][0]                \n__________________________________________________________________________________________________\nblock_1_depthwise_BN (BatchNorm (None, 32, 32, 96)   384         block_1_depthwise[0][0]          \n__________________________________________________________________________________________________\nblock_1_depthwise_relu (ReLU)   (None, 32, 32, 96)   0           block_1_depthwise_BN[0][0]       \n__________________________________________________________________________________________________\nblock_1_project (Conv2D)        (None, 32, 32, 24)   2304        block_1_depthwise_relu[0][0]     \n__________________________________________________________________________________________________\nblock_1_project_BN (BatchNormal (None, 32, 32, 24)   96          block_1_project[0][0]            \n__________________________________________________________________________________________________\nblock_2_expand (Conv2D)         (None, 32, 32, 144)  3456        block_1_project_BN[0][0]         \n__________________________________________________________________________________________________\nblock_2_expand_BN (BatchNormali (None, 32, 32, 144)  576         block_2_expand[0][0]             \n__________________________________________________________________________________________________\nblock_2_expand_relu (ReLU)      (None, 32, 32, 144)  0           block_2_expand_BN[0][0]          \n__________________________________________________________________________________________________\nblock_2_depthwise (DepthwiseCon (None, 32, 32, 144)  1296        block_2_expand_relu[0][0]        \n__________________________________________________________________________________________________\nblock_2_depthwise_BN (BatchNorm (None, 32, 32, 144)  576         block_2_depthwise[0][0]          \n__________________________________________________________________________________________________\nblock_2_depthwise_relu (ReLU)   (None, 32, 32, 144)  0           block_2_depthwise_BN[0][0]       \n__________________________________________________________________________________________________\nblock_2_project (Conv2D)        (None, 32, 32, 24)   3456        block_2_depthwise_relu[0][0]     \n__________________________________________________________________________________________________\nblock_2_project_BN (BatchNormal (None, 32, 32, 24)   96          block_2_project[0][0]            \n__________________________________________________________________________________________________\nblock_2_add (Add)               (None, 32, 32, 24)   0           block_1_project_BN[0][0]         \n                                                                 block_2_project_BN[0][0]         \n__________________________________________________________________________________________________\nblock_3_expand (Conv2D)         (None, 32, 32, 144)  3456        block_2_add[0][0]                \n__________________________________________________________________________________________________\nblock_3_expand_BN (BatchNormali (None, 32, 32, 144)  576         block_3_expand[0][0]             \n__________________________________________________________________________________________________\nblock_3_expand_relu (ReLU)      (None, 32, 32, 144)  0           block_3_expand_BN[0][0]          \n__________________________________________________________________________________________________\nblock_3_pad (ZeroPadding2D)     (None, 33, 33, 144)  0           block_3_expand_relu[0][0]        \n__________________________________________________________________________________________________\nblock_3_depthwise (DepthwiseCon (None, 16, 16, 144)  1296        block_3_pad[0][0]                \n__________________________________________________________________________________________________\nblock_3_depthwise_BN (BatchNorm (None, 16, 16, 144)  576         block_3_depthwise[0][0]          \n__________________________________________________________________________________________________\nblock_3_depthwise_relu (ReLU)   (None, 16, 16, 144)  0           block_3_depthwise_BN[0][0]       \n__________________________________________________________________________________________________\nblock_3_project (Conv2D)        (None, 16, 16, 32)   4608        block_3_depthwise_relu[0][0]     \n__________________________________________________________________________________________________\nblock_3_project_BN (BatchNormal (None, 16, 16, 32)   128         block_3_project[0][0]            \n__________________________________________________________________________________________________\nblock_4_expand (Conv2D)         (None, 16, 16, 192)  6144        block_3_project_BN[0][0]         \n__________________________________________________________________________________________________\nblock_4_expand_BN (BatchNormali (None, 16, 16, 192)  768         block_4_expand[0][0]             \n__________________________________________________________________________________________________\nblock_4_expand_relu (ReLU)      (None, 16, 16, 192)  0           block_4_expand_BN[0][0]          \n__________________________________________________________________________________________________\nblock_4_depthwise (DepthwiseCon (None, 16, 16, 192)  1728        block_4_expand_relu[0][0]        \n__________________________________________________________________________________________________\nblock_4_depthwise_BN (BatchNorm (None, 16, 16, 192)  768         block_4_depthwise[0][0]          \n__________________________________________________________________________________________________\nblock_4_depthwise_relu (ReLU)   (None, 16, 16, 192)  0           block_4_depthwise_BN[0][0]       \n__________________________________________________________________________________________________\nblock_4_project (Conv2D)        (None, 16, 16, 32)   6144        block_4_depthwise_relu[0][0]     \n__________________________________________________________________________________________________\nblock_4_project_BN (BatchNormal (None, 16, 16, 32)   128         block_4_project[0][0]            \n__________________________________________________________________________________________________\nblock_4_add (Add)               (None, 16, 16, 32)   0           block_3_project_BN[0][0]         \n                                                                 block_4_project_BN[0][0]         \n__________________________________________________________________________________________________\nblock_5_expand (Conv2D)         (None, 16, 16, 192)  6144        block_4_add[0][0]                \n__________________________________________________________________________________________________\nblock_5_expand_BN (BatchNormali (None, 16, 16, 192)  768         block_5_expand[0][0]             \n__________________________________________________________________________________________________\nblock_5_expand_relu (ReLU)      (None, 16, 16, 192)  0           block_5_expand_BN[0][0]          \n__________________________________________________________________________________________________\nblock_5_depthwise (DepthwiseCon (None, 16, 16, 192)  1728        block_5_expand_relu[0][0]        \n__________________________________________________________________________________________________\nblock_5_depthwise_BN (BatchNorm (None, 16, 16, 192)  768         block_5_depthwise[0][0]          \n__________________________________________________________________________________________________\nblock_5_depthwise_relu (ReLU)   (None, 16, 16, 192)  0           block_5_depthwise_BN[0][0]       \n__________________________________________________________________________________________________\nblock_5_project (Conv2D)        (None, 16, 16, 32)   6144        block_5_depthwise_relu[0][0]     \n__________________________________________________________________________________________________\nblock_5_project_BN (BatchNormal (None, 16, 16, 32)   128         block_5_project[0][0]            \n__________________________________________________________________________________________________\nblock_5_add (Add)               (None, 16, 16, 32)   0           block_4_add[0][0]                \n                                                                 block_5_project_BN[0][0]         \n__________________________________________________________________________________________________\nblock_6_expand (Conv2D)         (None, 16, 16, 192)  6144        block_5_add[0][0]                \n__________________________________________________________________________________________________\nblock_6_expand_BN (BatchNormali (None, 16, 16, 192)  768         block_6_expand[0][0]             \n__________________________________________________________________________________________________\nblock_6_expand_relu (ReLU)      (None, 16, 16, 192)  0           block_6_expand_BN[0][0]          \n__________________________________________________________________________________________________\nblock_6_pad (ZeroPadding2D)     (None, 17, 17, 192)  0           block_6_expand_relu[0][0]        \n__________________________________________________________________________________________________\nblock_6_depthwise (DepthwiseCon (None, 8, 8, 192)    1728        block_6_pad[0][0]                \n__________________________________________________________________________________________________\nblock_6_depthwise_BN (BatchNorm (None, 8, 8, 192)    768         block_6_depthwise[0][0]          \n__________________________________________________________________________________________________\nblock_6_depthwise_relu (ReLU)   (None, 8, 8, 192)    0           block_6_depthwise_BN[0][0]       \n__________________________________________________________________________________________________\nblock_6_project (Conv2D)        (None, 8, 8, 64)     12288       block_6_depthwise_relu[0][0]     \n__________________________________________________________________________________________________\nblock_6_project_BN (BatchNormal (None, 8, 8, 64)     256         block_6_project[0][0]            \n__________________________________________________________________________________________________\nblock_7_expand (Conv2D)         (None, 8, 8, 384)    24576       block_6_project_BN[0][0]         \n__________________________________________________________________________________________________\nblock_7_expand_BN (BatchNormali (None, 8, 8, 384)    1536        block_7_expand[0][0]             \n__________________________________________________________________________________________________\nblock_7_expand_relu (ReLU)      (None, 8, 8, 384)    0           block_7_expand_BN[0][0]          \n__________________________________________________________________________________________________\nblock_7_depthwise (DepthwiseCon (None, 8, 8, 384)    3456        block_7_expand_relu[0][0]        \n__________________________________________________________________________________________________\nblock_7_depthwise_BN (BatchNorm (None, 8, 8, 384)    1536        block_7_depthwise[0][0]          \n__________________________________________________________________________________________________\nblock_7_depthwise_relu (ReLU)   (None, 8, 8, 384)    0           block_7_depthwise_BN[0][0]       \n__________________________________________________________________________________________________\nblock_7_project (Conv2D)        (None, 8, 8, 64)     24576       block_7_depthwise_relu[0][0]     \n__________________________________________________________________________________________________\nblock_7_project_BN (BatchNormal (None, 8, 8, 64)     256         block_7_project[0][0]            \n__________________________________________________________________________________________________\nblock_7_add (Add)               (None, 8, 8, 64)     0           block_6_project_BN[0][0]         \n                                                                 block_7_project_BN[0][0]         \n__________________________________________________________________________________________________\nblock_8_expand (Conv2D)         (None, 8, 8, 384)    24576       block_7_add[0][0]                \n__________________________________________________________________________________________________\nblock_8_expand_BN (BatchNormali (None, 8, 8, 384)    1536        block_8_expand[0][0]             \n__________________________________________________________________________________________________\nblock_8_expand_relu (ReLU)      (None, 8, 8, 384)    0           block_8_expand_BN[0][0]          \n__________________________________________________________________________________________________\nblock_8_depthwise (DepthwiseCon (None, 8, 8, 384)    3456        block_8_expand_relu[0][0]        \n__________________________________________________________________________________________________\nblock_8_depthwise_BN (BatchNorm (None, 8, 8, 384)    1536        block_8_depthwise[0][0]          \n__________________________________________________________________________________________________\nblock_8_depthwise_relu (ReLU)   (None, 8, 8, 384)    0           block_8_depthwise_BN[0][0]       \n__________________________________________________________________________________________________\nblock_8_project (Conv2D)        (None, 8, 8, 64)     24576       block_8_depthwise_relu[0][0]     \n__________________________________________________________________________________________________\nblock_8_project_BN (BatchNormal (None, 8, 8, 64)     256         block_8_project[0][0]            \n__________________________________________________________________________________________________\nblock_8_add (Add)               (None, 8, 8, 64)     0           block_7_add[0][0]                \n                                                                 block_8_project_BN[0][0]         \n__________________________________________________________________________________________________\nblock_9_expand (Conv2D)         (None, 8, 8, 384)    24576       block_8_add[0][0]                \n__________________________________________________________________________________________________\nblock_9_expand_BN (BatchNormali (None, 8, 8, 384)    1536        block_9_expand[0][0]             \n__________________________________________________________________________________________________\nblock_9_expand_relu (ReLU)      (None, 8, 8, 384)    0           block_9_expand_BN[0][0]          \n__________________________________________________________________________________________________\nblock_9_depthwise (DepthwiseCon (None, 8, 8, 384)    3456        block_9_expand_relu[0][0]        \n__________________________________________________________________________________________________\nblock_9_depthwise_BN (BatchNorm (None, 8, 8, 384)    1536        block_9_depthwise[0][0]          \n__________________________________________________________________________________________________\nblock_9_depthwise_relu (ReLU)   (None, 8, 8, 384)    0           block_9_depthwise_BN[0][0]       \n__________________________________________________________________________________________________\nblock_9_project (Conv2D)        (None, 8, 8, 64)     24576       block_9_depthwise_relu[0][0]     \n__________________________________________________________________________________________________\nblock_9_project_BN (BatchNormal (None, 8, 8, 64)     256         block_9_project[0][0]            \n__________________________________________________________________________________________________\nblock_9_add (Add)               (None, 8, 8, 64)     0           block_8_add[0][0]                \n                                                                 block_9_project_BN[0][0]         \n__________________________________________________________________________________________________\nblock_10_expand (Conv2D)        (None, 8, 8, 384)    24576       block_9_add[0][0]                \n__________________________________________________________________________________________________\nblock_10_expand_BN (BatchNormal (None, 8, 8, 384)    1536        block_10_expand[0][0]            \n__________________________________________________________________________________________________\nblock_10_expand_relu (ReLU)     (None, 8, 8, 384)    0           block_10_expand_BN[0][0]         \n__________________________________________________________________________________________________\nblock_10_depthwise (DepthwiseCo (None, 8, 8, 384)    3456        block_10_expand_relu[0][0]       \n__________________________________________________________________________________________________\nblock_10_depthwise_BN (BatchNor (None, 8, 8, 384)    1536        block_10_depthwise[0][0]         \n__________________________________________________________________________________________________\nblock_10_depthwise_relu (ReLU)  (None, 8, 8, 384)    0           block_10_depthwise_BN[0][0]      \n__________________________________________________________________________________________________\nblock_10_project (Conv2D)       (None, 8, 8, 96)     36864       block_10_depthwise_relu[0][0]    \n__________________________________________________________________________________________________\nblock_10_project_BN (BatchNorma (None, 8, 8, 96)     384         block_10_project[0][0]           \n__________________________________________________________________________________________________\nblock_11_expand (Conv2D)        (None, 8, 8, 576)    55296       block_10_project_BN[0][0]        \n__________________________________________________________________________________________________\nblock_11_expand_BN (BatchNormal (None, 8, 8, 576)    2304        block_11_expand[0][0]            \n__________________________________________________________________________________________________\nblock_11_expand_relu (ReLU)     (None, 8, 8, 576)    0           block_11_expand_BN[0][0]         \n__________________________________________________________________________________________________\nblock_11_depthwise (DepthwiseCo (None, 8, 8, 576)    5184        block_11_expand_relu[0][0]       \n__________________________________________________________________________________________________\nblock_11_depthwise_BN (BatchNor (None, 8, 8, 576)    2304        block_11_depthwise[0][0]         \n__________________________________________________________________________________________________\nblock_11_depthwise_relu (ReLU)  (None, 8, 8, 576)    0           block_11_depthwise_BN[0][0]      \n__________________________________________________________________________________________________\nblock_11_project (Conv2D)       (None, 8, 8, 96)     55296       block_11_depthwise_relu[0][0]    \n__________________________________________________________________________________________________\nblock_11_project_BN (BatchNorma (None, 8, 8, 96)     384         block_11_project[0][0]           \n__________________________________________________________________________________________________\nblock_11_add (Add)              (None, 8, 8, 96)     0           block_10_project_BN[0][0]        \n                                                                 block_11_project_BN[0][0]        \n__________________________________________________________________________________________________\nblock_12_expand (Conv2D)        (None, 8, 8, 576)    55296       block_11_add[0][0]               \n__________________________________________________________________________________________________\nblock_12_expand_BN (BatchNormal (None, 8, 8, 576)    2304        block_12_expand[0][0]            \n__________________________________________________________________________________________________\nblock_12_expand_relu (ReLU)     (None, 8, 8, 576)    0           block_12_expand_BN[0][0]         \n__________________________________________________________________________________________________\nblock_12_depthwise (DepthwiseCo (None, 8, 8, 576)    5184        block_12_expand_relu[0][0]       \n__________________________________________________________________________________________________\nblock_12_depthwise_BN (BatchNor (None, 8, 8, 576)    2304        block_12_depthwise[0][0]         \n__________________________________________________________________________________________________\nblock_12_depthwise_relu (ReLU)  (None, 8, 8, 576)    0           block_12_depthwise_BN[0][0]      \n__________________________________________________________________________________________________\nblock_12_project (Conv2D)       (None, 8, 8, 96)     55296       block_12_depthwise_relu[0][0]    \n__________________________________________________________________________________________________\nblock_12_project_BN (BatchNorma (None, 8, 8, 96)     384         block_12_project[0][0]           \n__________________________________________________________________________________________________\nblock_12_add (Add)              (None, 8, 8, 96)     0           block_11_add[0][0]               \n                                                                 block_12_project_BN[0][0]        \n__________________________________________________________________________________________________\nblock_13_expand (Conv2D)        (None, 8, 8, 576)    55296       block_12_add[0][0]               \n__________________________________________________________________________________________________\nblock_13_expand_BN (BatchNormal (None, 8, 8, 576)    2304        block_13_expand[0][0]            \n__________________________________________________________________________________________________\nblock_13_expand_relu (ReLU)     (None, 8, 8, 576)    0           block_13_expand_BN[0][0]         \n__________________________________________________________________________________________________\nblock_13_pad (ZeroPadding2D)    (None, 9, 9, 576)    0           block_13_expand_relu[0][0]       \n__________________________________________________________________________________________________\nblock_13_depthwise (DepthwiseCo (None, 4, 4, 576)    5184        block_13_pad[0][0]               \n__________________________________________________________________________________________________\nblock_13_depthwise_BN (BatchNor (None, 4, 4, 576)    2304        block_13_depthwise[0][0]         \n__________________________________________________________________________________________________\nblock_13_depthwise_relu (ReLU)  (None, 4, 4, 576)    0           block_13_depthwise_BN[0][0]      \n__________________________________________________________________________________________________\nblock_13_project (Conv2D)       (None, 4, 4, 160)    92160       block_13_depthwise_relu[0][0]    \n__________________________________________________________________________________________________\nblock_13_project_BN (BatchNorma (None, 4, 4, 160)    640         block_13_project[0][0]           \n__________________________________________________________________________________________________\nblock_14_expand (Conv2D)        (None, 4, 4, 960)    153600      block_13_project_BN[0][0]        \n__________________________________________________________________________________________________\nblock_14_expand_BN (BatchNormal (None, 4, 4, 960)    3840        block_14_expand[0][0]            \n__________________________________________________________________________________________________\nblock_14_expand_relu (ReLU)     (None, 4, 4, 960)    0           block_14_expand_BN[0][0]         \n__________________________________________________________________________________________________\nblock_14_depthwise (DepthwiseCo (None, 4, 4, 960)    8640        block_14_expand_relu[0][0]       \n__________________________________________________________________________________________________\nblock_14_depthwise_BN (BatchNor (None, 4, 4, 960)    3840        block_14_depthwise[0][0]         \n__________________________________________________________________________________________________\nblock_14_depthwise_relu (ReLU)  (None, 4, 4, 960)    0           block_14_depthwise_BN[0][0]      \n__________________________________________________________________________________________________\nblock_14_project (Conv2D)       (None, 4, 4, 160)    153600      block_14_depthwise_relu[0][0]    \n__________________________________________________________________________________________________\nblock_14_project_BN (BatchNorma (None, 4, 4, 160)    640         block_14_project[0][0]           \n__________________________________________________________________________________________________\nblock_14_add (Add)              (None, 4, 4, 160)    0           block_13_project_BN[0][0]        \n                                                                 block_14_project_BN[0][0]        \n__________________________________________________________________________________________________\nblock_15_expand (Conv2D)        (None, 4, 4, 960)    153600      block_14_add[0][0]               \n__________________________________________________________________________________________________\nblock_15_expand_BN (BatchNormal (None, 4, 4, 960)    3840        block_15_expand[0][0]            \n__________________________________________________________________________________________________\nblock_15_expand_relu (ReLU)     (None, 4, 4, 960)    0           block_15_expand_BN[0][0]         \n__________________________________________________________________________________________________\nblock_15_depthwise (DepthwiseCo (None, 4, 4, 960)    8640        block_15_expand_relu[0][0]       \n__________________________________________________________________________________________________\nblock_15_depthwise_BN (BatchNor (None, 4, 4, 960)    3840        block_15_depthwise[0][0]         \n__________________________________________________________________________________________________\nblock_15_depthwise_relu (ReLU)  (None, 4, 4, 960)    0           block_15_depthwise_BN[0][0]      \n__________________________________________________________________________________________________\nblock_15_project (Conv2D)       (None, 4, 4, 160)    153600      block_15_depthwise_relu[0][0]    \n__________________________________________________________________________________________________\nblock_15_project_BN (BatchNorma (None, 4, 4, 160)    640         block_15_project[0][0]           \n__________________________________________________________________________________________________\nblock_15_add (Add)              (None, 4, 4, 160)    0           block_14_add[0][0]               \n                                                                 block_15_project_BN[0][0]        \n__________________________________________________________________________________________________\nblock_16_expand (Conv2D)        (None, 4, 4, 960)    153600      block_15_add[0][0]               \n__________________________________________________________________________________________________\nblock_16_expand_BN (BatchNormal (None, 4, 4, 960)    3840        block_16_expand[0][0]            \n__________________________________________________________________________________________________\nblock_16_expand_relu (ReLU)     (None, 4, 4, 960)    0           block_16_expand_BN[0][0]         \n__________________________________________________________________________________________________\nblock_16_depthwise (DepthwiseCo (None, 4, 4, 960)    8640        block_16_expand_relu[0][0]       \n__________________________________________________________________________________________________\nblock_16_depthwise_BN (BatchNor (None, 4, 4, 960)    3840        block_16_depthwise[0][0]         \n__________________________________________________________________________________________________\nblock_16_depthwise_relu (ReLU)  (None, 4, 4, 960)    0           block_16_depthwise_BN[0][0]      \n__________________________________________________________________________________________________\nblock_16_project (Conv2D)       (None, 4, 4, 320)    307200      block_16_depthwise_relu[0][0]    \n__________________________________________________________________________________________________\nblock_16_project_BN (BatchNorma (None, 4, 4, 320)    1280        block_16_project[0][0]           \n__________________________________________________________________________________________________\nConv_1 (Conv2D)                 (None, 4, 4, 1280)   409600      block_16_project_BN[0][0]        \n__________________________________________________________________________________________________\nConv_1_bn (BatchNormalization)  (None, 4, 4, 1280)   5120        Conv_1[0][0]                     \n__________________________________________________________________________________________________\nout_relu (ReLU)                 (None, 4, 4, 1280)   0           Conv_1_bn[0][0]                  \n==================================================================================================\nTotal params: 2,257,984\nTrainable params: 0\nNon-trainable params: 2,257,984\n__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Freezing (by setting layer.trainable = False) prevents the weights in a given layer from being updated during training.\n",
    "base_model.trainable = False\n",
    "base_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(256, 4, 4, 1280)\n"
     ]
    }
   ],
   "source": [
    "#This feature extractor converts each 128x128x3 image into a 5x5x1280 block of features. Let's see what it does to an example batch of images. (Batch size being 256)\n",
    "\n",
    "image_batch, label_batch = next(iter(train))\n",
    "feature_batch = base_model(image_batch)\n",
    "print(feature_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(256, 1280)\n"
     ]
    }
   ],
   "source": [
    "# Add a classification head\n",
    "# To generate predictions from the block of features, average over the spatial 5x5 spatial locations, using a tf.keras.layers.GlobalAveragePooling2D layer to convert the features to a single 1280-element vector per image.\n",
    "\n",
    "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "feature_batch_average = global_average_layer(feature_batch)\n",
    "print(feature_batch_average.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(256, 1)\n"
     ]
    }
   ],
   "source": [
    "# Apply a tf.keras.layers.Dense layer to convert these features into a single prediction per image. You don't need an activation function here because this prediction will be treated as a logit, or a raw prediction value. Positive numbers predict class 1, negative numbers predict class 0.\n",
    "\n",
    "# prediction_layer = tf.keras.layers.Dense(1)\n",
    "prediction_layer = tf.keras.layers.Dense(1,activation='sigmoid')\n",
    "prediction_batch = prediction_layer(feature_batch_average)\n",
    "print(prediction_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a model by chaining together the rescaling, base_model and feature extractor layers using the Keras Functional API. As previously mentioned, use training=False as our model contains a BatchNormalization layer.\n",
    "\n",
    "inputs = tf.keras.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "x = preprocess_input(inputs)\n",
    "x = base_model(x, training=False)\n",
    "x = global_average_layer(x)\n",
    "x = tf.keras.layers.Dropout(0.4)(x)\n",
    "outputs = prediction_layer(x)\n",
    "model = tf.keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"functional_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_2 (InputLayer)         [(None, 128, 128, 3)]     0         \n_________________________________________________________________\ntf_op_layer_RealDiv (TensorF [(None, 128, 128, 3)]     0         \n_________________________________________________________________\ntf_op_layer_Sub (TensorFlowO [(None, 128, 128, 3)]     0         \n_________________________________________________________________\nmobilenetv2_1.00_128 (Functi (None, 4, 4, 1280)        2257984   \n_________________________________________________________________\nglobal_average_pooling2d (Gl (None, 1280)              0         \n_________________________________________________________________\ndropout (Dropout)            (None, 1280)              0         \n_________________________________________________________________\ndense (Dense)                (None, 1)                 1281      \n=================================================================\nTotal params: 2,259,265\nTrainable params: 1,281\nNon-trainable params: 2,257,984\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Compile the model before training it. Since there are two classes, use a binary cross-entropy loss with from_logits=True since the model provides a linear output.\n",
    "\n",
    "METRICS = [keras.metrics.Precision(name='precision'),keras.metrics.Recall(name='recall'),keras.metrics.AUC(name='auc'),]\n",
    "\n",
    "base_learning_rate = 1e-4\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=base_learning_rate),\n",
    "            #   loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              loss=tf.keras.losses.binary_crossentropy,\n",
    "              metrics=METRICS)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "# The 2.5M parameters in MobileNet are frozen, but there are 1.2K trainable parameters in the Dense layer. These are divided between two tf.Variable objects, the weights and biases.\n",
    "\n",
    "len(model.trainable_variables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "weights: 262\ntrainable_weights: 2\nnon_trainable_weights: 260\n"
     ]
    }
   ],
   "source": [
    "print(\"weights:\", len(model.weights))\n",
    "print(\"trainable_weights:\", len(model.trainable_weights))\n",
    "print(\"non_trainable_weights:\", len(model.non_trainable_weights))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Failed to query for notebook name, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mamitagni\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.10 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                Tracking run with wandb version 0.10.8<br/>\n                Syncing run <strong style=\"color:#cdcd00\">TL50FilesShuffle10KSteps400_AdamLR1e-4_Base</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://wandb.ai/amitagni/candlestick-CNN\" target=\"_blank\">https://wandb.ai/amitagni/candlestick-CNN</a><br/>\n                Run page: <a href=\"https://wandb.ai/amitagni/candlestick-CNN/runs/mkbs1wyl\" target=\"_blank\">https://wandb.ai/amitagni/candlestick-CNN/runs/mkbs1wyl</a><br/>\n                Run data is saved locally in <code>wandb/run-20201111_182059-mkbs1wyl</code><br/><br/>\n            "
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/100\n",
      "400/400 [==============================] - 31s 77ms/step - loss: 0.6843 - precision: 0.2268 - recall: 0.1435 - auc: 0.5002\n",
      "Epoch 2/100\n",
      "400/400 [==============================] - 31s 78ms/step - loss: 0.6434 - precision: 0.2569 - recall: 0.1025 - auc: 0.5084\n",
      "Epoch 3/100\n",
      "400/400 [==============================] - 31s 78ms/step - loss: 0.5998 - precision: 0.2363 - recall: 0.0573 - auc: 0.5091\n",
      "Epoch 4/100\n",
      "400/400 [==============================] - 31s 78ms/step - loss: 0.5939 - precision: 0.2713 - recall: 0.0448 - auc: 0.5140\n",
      "Epoch 5/100\n",
      "400/400 [==============================] - 34s 85ms/step - loss: 0.5663 - precision: 0.2471 - recall: 0.0172 - auc: 0.5189 - val_loss: 0.5754 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5085\n",
      "Epoch 6/100\n",
      "400/400 [==============================] - 31s 78ms/step - loss: 0.5693 - precision: 0.3092 - recall: 0.0136 - auc: 0.5248\n",
      "Epoch 7/100\n",
      "400/400 [==============================] - 31s 78ms/step - loss: 0.5552 - precision: 0.2775 - recall: 0.0040 - auc: 0.5217\n",
      "Epoch 8/100\n",
      "400/400 [==============================] - 31s 78ms/step - loss: 0.5621 - precision: 0.2965 - recall: 0.0027 - auc: 0.5249\n",
      "Epoch 9/100\n",
      "400/400 [==============================] - 31s 78ms/step - loss: 0.5489 - precision: 0.3043 - recall: 0.0012 - auc: 0.5291\n",
      "Epoch 10/100\n",
      "400/400 [==============================] - 33s 83ms/step - loss: 0.5560 - precision: 0.3651 - recall: 9.2911e-04 - auc: 0.5320 - val_loss: 0.5491 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5109\n",
      "Epoch 11/100\n",
      "400/400 [==============================] - 31s 78ms/step - loss: 0.5478 - precision: 0.3462 - recall: 3.7262e-04 - auc: 0.5366\n",
      "Epoch 12/100\n",
      "400/400 [==============================] - 31s 78ms/step - loss: 0.5524 - precision: 0.3846 - recall: 2.0254e-04 - auc: 0.5379\n",
      "Epoch 13/100\n",
      "400/400 [==============================] - 31s 78ms/step - loss: 0.5469 - precision: 0.3077 - recall: 1.6534e-04 - auc: 0.5407\n",
      "Epoch 14/100\n",
      "400/400 [==============================] - 31s 78ms/step - loss: 0.5515 - precision: 0.4545 - recall: 2.0299e-04 - auc: 0.5379\n",
      "Epoch 15/100\n",
      "400/400 [==============================] - 33s 82ms/step - loss: 0.5464 - precision: 0.5000 - recall: 8.2481e-05 - auc: 0.5436 - val_loss: 0.5554 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5350\n",
      "Epoch 16/100\n",
      "400/400 [==============================] - 31s 78ms/step - loss: 0.5502 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5402\n",
      "Epoch 17/100\n",
      "400/400 [==============================] - 31s 78ms/step - loss: 0.5458 - precision: 0.3333 - recall: 4.1212e-05 - auc: 0.5488\n",
      "Epoch 18/100\n",
      "400/400 [==============================] - 31s 78ms/step - loss: 0.5489 - precision: 0.5000 - recall: 4.0698e-05 - auc: 0.5448\n",
      "Epoch 19/100\n",
      "400/400 [==============================] - 31s 78ms/step - loss: 0.5451 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5487\n",
      "Epoch 20/100\n",
      "400/400 [==============================] - 33s 82ms/step - loss: 0.5486 - precision: 0.5000 - recall: 4.0771e-05 - auc: 0.5409 - val_loss: 0.5550 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5198\n",
      "Epoch 21/100\n",
      "400/400 [==============================] - 31s 78ms/step - loss: 0.5453 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5497\n",
      "Epoch 22/100\n",
      "400/400 [==============================] - 31s 78ms/step - loss: 0.5476 - precision: 1.0000 - recall: 4.0840e-05 - auc: 0.5435\n",
      "Epoch 23/100\n",
      "400/400 [==============================] - 31s 78ms/step - loss: 0.5460 - precision: 1.0000 - recall: 4.1066e-05 - auc: 0.5512\n",
      "Epoch 24/100\n",
      "400/400 [==============================] - 31s 78ms/step - loss: 0.5459 - precision: 1.0000 - recall: 4.1032e-05 - auc: 0.5472\n",
      "Epoch 25/100\n",
      "400/400 [==============================] - 33s 82ms/step - loss: 0.5452 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5522 - val_loss: 0.5760 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5434\n",
      "Epoch 26/100\n",
      "400/400 [==============================] - 31s 78ms/step - loss: 0.5473 - precision: 1.0000 - recall: 4.0856e-05 - auc: 0.5445\n",
      "Epoch 27/100\n",
      "400/400 [==============================] - 31s 78ms/step - loss: 0.5439 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5514\n",
      "Epoch 28/100\n",
      "400/400 [==============================] - 31s 78ms/step - loss: 0.5478 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5431\n",
      "Epoch 29/100\n",
      "400/400 [==============================] - 31s 78ms/step - loss: 0.5443 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5541\n",
      "Epoch 30/100\n",
      "400/400 [==============================] - 33s 83ms/step - loss: 0.5472 - precision: 0.7500 - recall: 1.2254e-04 - auc: 0.5470 - val_loss: 0.5469 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5293\n",
      "Epoch 31/100\n",
      "400/400 [==============================] - 31s 78ms/step - loss: 0.5448 - precision: 1.0000 - recall: 4.1229e-05 - auc: 0.5513\n",
      "Epoch 32/100\n",
      "400/400 [==============================] - 31s 78ms/step - loss: 0.5468 - precision: 0.5000 - recall: 4.0915e-05 - auc: 0.5465\n",
      "Epoch 33/100\n",
      "400/400 [==============================] - 31s 78ms/step - loss: 0.5446 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5533\n",
      "Epoch 34/100\n",
      "400/400 [==============================] - 31s 78ms/step - loss: 0.5454 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5490\n",
      "Epoch 35/100\n",
      "400/400 [==============================] - 33s 83ms/step - loss: 0.5471 - precision: 0.5000 - recall: 4.0780e-05 - auc: 0.5557 - val_loss: 0.5468 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5366\n",
      "Epoch 36/100\n",
      "400/400 [==============================] - 31s 78ms/step - loss: 0.5437 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5482\n",
      "Epoch 37/100\n",
      "400/400 [==============================] - 31s 78ms/step - loss: 0.5475 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5548\n",
      "Epoch 38/100\n",
      "400/400 [==============================] - 31s 78ms/step - loss: 0.5434 - precision: 0.5000 - recall: 4.1292e-05 - auc: 0.5529\n",
      "Epoch 39/100\n",
      "400/400 [==============================] - 31s 78ms/step - loss: 0.5478 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5523\n",
      "Epoch 40/100\n",
      "400/400 [==============================] - 33s 81ms/step - loss: 0.5423 - precision: 0.3333 - recall: 4.1511e-05 - auc: 0.5496 - val_loss: 0.5590 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5382\n",
      "Epoch 41/100\n",
      "400/400 [==============================] - 31s 78ms/step - loss: 0.5485 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5530\n",
      "Epoch 42/100\n",
      "400/400 [==============================] - 31s 78ms/step - loss: 0.5415 - precision: 1.0000 - recall: 4.1599e-05 - auc: 0.5518\n",
      "Epoch 43/100\n",
      "400/400 [==============================] - 31s 78ms/step - loss: 0.5494 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5511\n",
      "Epoch 44/100\n",
      "400/400 [==============================] - 31s 78ms/step - loss: 0.5395 - precision: 0.6667 - recall: 8.3703e-05 - auc: 0.5532\n",
      "Epoch 45/100\n",
      "400/400 [==============================] - 33s 82ms/step - loss: 0.5521 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5470 - val_loss: 0.5604 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5481\n",
      "Epoch 46/100\n",
      "400/400 [==============================] - 31s 78ms/step - loss: 0.5383 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5555\n",
      "Epoch 47/100\n",
      "400/400 [==============================] - 31s 78ms/step - loss: 0.5529 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5478\n",
      "Epoch 48/100\n",
      "400/400 [==============================] - 31s 78ms/step - loss: 0.5372 - precision: 0.3333 - recall: 4.2164e-05 - auc: 0.5556\n",
      "Epoch 49/100\n",
      "400/400 [==============================] - 31s 78ms/step - loss: 0.5532 - precision: 1.0000 - recall: 8.0035e-05 - auc: 0.5478\n",
      "Epoch 50/100\n",
      "400/400 [==============================] - 33s 82ms/step - loss: 0.5373 - precision: 0.7500 - recall: 1.2644e-04 - auc: 0.5557 - val_loss: 0.5628 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5301\n",
      "Epoch 51/100\n",
      "400/400 [==============================] - 31s 78ms/step - loss: 0.5546 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5478\n",
      "Epoch 52/100\n",
      "400/400 [==============================] - 31s 78ms/step - loss: 0.5361 - precision: 0.5000 - recall: 8.4645e-05 - auc: 0.5554\n",
      "Epoch 53/100\n",
      "400/400 [==============================] - 31s 78ms/step - loss: 0.5535 - precision: 1.0000 - recall: 3.9935e-05 - auc: 0.5495\n",
      "Epoch 54/100\n",
      "400/400 [==============================] - 31s 78ms/step - loss: 0.5371 - precision: 1.0000 - recall: 1.2652e-04 - auc: 0.5557\n",
      "Epoch 55/100\n",
      "400/400 [==============================] - 33s 83ms/step - loss: 0.5541 - precision: 0.5000 - recall: 3.9861e-05 - auc: 0.5485 - val_loss: 0.5459 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5528\n",
      "Epoch 56/100\n",
      "400/400 [==============================] - 31s 78ms/step - loss: 0.5362 - precision: 0.6667 - recall: 8.4567e-05 - auc: 0.5571\n",
      "Epoch 57/100\n",
      "400/400 [==============================] - 31s 78ms/step - loss: 0.5551 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5475\n",
      "Epoch 58/100\n",
      "400/400 [==============================] - 31s 78ms/step - loss: 0.5361 - precision: 0.6667 - recall: 8.4670e-05 - auc: 0.5564\n",
      "Epoch 59/100\n",
      "400/400 [==============================] - 31s 78ms/step - loss: 0.5542 - precision: 1.0000 - recall: 3.9869e-05 - auc: 0.5476\n",
      "Epoch 60/100\n",
      "400/400 [==============================] - 33s 81ms/step - loss: 0.5361 - precision: 0.6667 - recall: 1.6947e-04 - auc: 0.5552 - val_loss: 0.5713 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5344\n",
      "Epoch 61/100\n",
      "400/400 [==============================] - 31s 78ms/step - loss: 0.5533 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5480\n",
      "Epoch 62/100\n",
      "400/400 [==============================] - 31s 78ms/step - loss: 0.5375 - precision: 0.6667 - recall: 1.6878e-04 - auc: 0.5548\n",
      "Epoch 63/100\n",
      "400/400 [==============================] - 31s 78ms/step - loss: 0.5516 - precision: 1.0000 - recall: 4.0130e-05 - auc: 0.5518\n",
      "Epoch 64/100\n",
      "400/400 [==============================] - 31s 78ms/step - loss: 0.5394 - precision: 0.6667 - recall: 1.6778e-04 - auc: 0.5530\n",
      "Epoch 65/100\n",
      "400/400 [==============================] - 33s 82ms/step - loss: 0.5507 - precision: 0.4000 - recall: 8.0519e-05 - auc: 0.5508 - val_loss: 0.5474 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5313\n",
      "Epoch 66/100\n",
      "400/400 [==============================] - 31s 78ms/step - loss: 0.5392 - precision: 0.7500 - recall: 1.2588e-04 - auc: 0.5539\n",
      "Epoch 67/100\n",
      "400/400 [==============================] - 31s 78ms/step - loss: 0.5512 - precision: 0.2500 - recall: 4.0151e-05 - auc: 0.5524\n",
      "Epoch 68/100\n",
      "400/400 [==============================] - 31s 78ms/step - loss: 0.5392 - precision: 0.3333 - recall: 4.1957e-05 - auc: 0.5542\n",
      "Epoch 69/100\n",
      "400/400 [==============================] - 31s 78ms/step - loss: 0.5506 - precision: 1.0000 - recall: 1.2057e-04 - auc: 0.5548\n",
      "Epoch 70/100\n",
      "400/400 [==============================] - 33s 82ms/step - loss: 0.5404 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5522 - val_loss: 0.5656 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5328\n",
      "Epoch 71/100\n",
      "400/400 [==============================] - 31s 78ms/step - loss: 0.5499 - precision: 0.8000 - recall: 1.6113e-04 - auc: 0.5558\n",
      "Epoch 72/100\n",
      "400/400 [==============================] - 31s 78ms/step - loss: 0.5420 - precision: 1.0000 - recall: 8.3132e-05 - auc: 0.5528\n",
      "Epoch 73/100\n",
      "400/400 [==============================] - 31s 78ms/step - loss: 0.5488 - precision: 1.0000 - recall: 2.8298e-04 - auc: 0.5554\n",
      "Epoch 74/100\n",
      "400/400 [==============================] - 31s 78ms/step - loss: 0.5430 - precision: 0.6667 - recall: 8.2788e-05 - auc: 0.5547\n",
      "Epoch 75/100\n",
      "400/400 [==============================] - 33s 81ms/step - loss: 0.5489 - precision: 0.8571 - recall: 2.4289e-04 - auc: 0.5523 - val_loss: 0.5703 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5393\n",
      "Epoch 76/100\n",
      "400/400 [==============================] - 31s 78ms/step - loss: 0.5433 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5552\n",
      "Epoch 77/100\n",
      "400/400 [==============================] - 31s 78ms/step - loss: 0.5484 - precision: 0.4444 - recall: 1.6207e-04 - auc: 0.5532\n",
      "Epoch 78/100\n",
      "400/400 [==============================] - 31s 78ms/step - loss: 0.5435 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5582\n",
      "Epoch 79/100\n",
      "400/400 [==============================] - 31s 78ms/step - loss: 0.5472 - precision: 0.5000 - recall: 1.2226e-04 - auc: 0.5503\n",
      "Epoch 80/100\n",
      "400/400 [==============================] - 33s 82ms/step - loss: 0.5445 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5555 - val_loss: 0.5633 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5157\n",
      "Epoch 81/100\n",
      "400/400 [==============================] - 31s 78ms/step - loss: 0.5466 - precision: 0.8333 - recall: 2.0410e-04 - auc: 0.5512\n",
      "Epoch 82/100\n",
      "400/400 [==============================] - 31s 78ms/step - loss: 0.5436 - precision: 0.5000 - recall: 4.1235e-05 - auc: 0.5577\n",
      "Epoch 83/100\n",
      "400/400 [==============================] - 31s 78ms/step - loss: 0.5472 - precision: 0.7500 - recall: 2.4495e-04 - auc: 0.5483\n",
      "Epoch 84/100\n",
      "400/400 [==============================] - 31s 78ms/step - loss: 0.5437 - precision: 0.5000 - recall: 4.1244e-05 - auc: 0.5566\n",
      "Epoch 85/100\n",
      "400/400 [==============================] - 33s 81ms/step - loss: 0.5469 - precision: 1.0000 - recall: 2.4494e-04 - auc: 0.5502 - val_loss: 0.5505 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5202\n",
      "Epoch 86/100\n",
      "400/400 [==============================] - 31s 78ms/step - loss: 0.5444 - precision: 1.0000 - recall: 8.2335e-05 - auc: 0.5546\n",
      "Epoch 87/100\n",
      "400/400 [==============================] - 31s 78ms/step - loss: 0.5460 - precision: 0.8000 - recall: 1.6379e-04 - auc: 0.5502\n",
      "Epoch 88/100\n",
      "400/400 [==============================] - 31s 78ms/step - loss: 0.5441 - precision: 1.0000 - recall: 4.1173e-05 - auc: 0.5577\n",
      "Epoch 89/100\n",
      "400/400 [==============================] - 31s 78ms/step - loss: 0.5463 - precision: 0.6667 - recall: 8.1786e-05 - auc: 0.5506\n",
      "Epoch 90/100\n",
      "400/400 [==============================] - 33s 81ms/step - loss: 0.5439 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5563 - val_loss: 0.5643 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5343\n",
      "Epoch 91/100\n",
      "400/400 [==============================] - 31s 78ms/step - loss: 0.5455 - precision: 0.7143 - recall: 2.0480e-04 - auc: 0.5528\n",
      "Epoch 92/100\n",
      "400/400 [==============================] - 31s 78ms/step - loss: 0.5443 - precision: 0.6667 - recall: 8.2217e-05 - auc: 0.5592\n",
      "Epoch 93/100\n",
      "400/400 [==============================] - 31s 78ms/step - loss: 0.5452 - precision: 0.8000 - recall: 1.6409e-04 - auc: 0.5529\n",
      "Epoch 94/100\n",
      "400/400 [==============================] - 31s 78ms/step - loss: 0.5444 - precision: 1.0000 - recall: 4.1093e-05 - auc: 0.5579\n",
      "Epoch 95/100\n",
      "400/400 [==============================] - 32s 81ms/step - loss: 0.5439 - precision: 0.8000 - recall: 1.6464e-04 - auc: 0.5552 - val_loss: 0.5661 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5296\n",
      "Epoch 96/100\n",
      "400/400 [==============================] - 31s 78ms/step - loss: 0.5466 - precision: 0.5000 - recall: 4.0810e-05 - auc: 0.5570\n",
      "Epoch 97/100\n",
      "400/400 [==============================] - 31s 78ms/step - loss: 0.5447 - precision: 0.7500 - recall: 2.4662e-04 - auc: 0.5514\n",
      "Epoch 98/100\n",
      "400/400 [==============================] - 31s 78ms/step - loss: 0.5468 - precision: 1.0000 - recall: 4.0868e-05 - auc: 0.5528\n",
      "Epoch 99/100\n",
      "400/400 [==============================] - 31s 78ms/step - loss: 0.5430 - precision: 0.8000 - recall: 3.2979e-04 - auc: 0.5569\n",
      "Epoch 100/100\n",
      "400/400 [==============================] - 33s 82ms/step - loss: 0.5469 - precision: 1.0000 - recall: 8.1686e-05 - auc: 0.5537 - val_loss: 0.5430 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5583\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 823<br/>Program ended successfully."
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "VBox(children=(Label(value=' 0.00MB of 0.10MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=0.02178248197…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a980ae7be62e4f2294108ec6ad82f745"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find user logs for this run at: <code>wandb/run-20201111_182059-mkbs1wyl/logs/debug.log</code>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find internal logs for this run at: <code>wandb/run-20201111_182059-mkbs1wyl/logs/debug-internal.log</code>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<h3>Run summary:</h3><br/><style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    </style><table class=\"wandb\">\n<tr><td>epoch</td><td>99</td></tr><tr><td>loss</td><td>0.54686</td></tr><tr><td>precision</td><td>1.0</td></tr><tr><td>recall</td><td>8e-05</td></tr><tr><td>auc</td><td>0.55366</td></tr><tr><td>_step</td><td>99</td></tr><tr><td>_runtime</td><td>3165</td></tr><tr><td>_timestamp</td><td>1605122025</td></tr><tr><td>val_loss</td><td>0.54299</td></tr><tr><td>val_precision</td><td>0.0</td></tr><tr><td>val_recall</td><td>0.0</td></tr><tr><td>val_auc</td><td>0.55827</td></tr><tr><td>best_val_loss</td><td>0.54299</td></tr><tr><td>best_epoch</td><td>99</td></tr></table>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<h3>Run history:</h3><br/><style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    </style><table class=\"wandb\">\n<tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>█▄▃▂▂▂▂▂▁▁▂▂▁▁▁▂▂▁▁▂▂▁▁▂▂▁▂▂▁▁▂▂▁▁▁▁▁▁▁▂</td></tr><tr><td>precision</td><td>▃▃▃▃▃▃▁▅▁██▁█▁▁▁▁▆▁█▁█▆█▁▆▃██▆▄▅▅▅▇▆▆█▆█</td></tr><tr><td>recall</td><td>█▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>auc</td><td>▁▂▄▄▅▆▆▆▇▇▆▆▇▇▇▇▇▇█▇▇██▇▇▇▇▇▇▇▇▇██▇▇██▇▇</td></tr><tr><td>_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_runtime</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▂▄▄█▂▂▄▅▅▂▇▂▆▇▅▃▆▆▁</td></tr><tr><td>val_precision</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_recall</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▁▁▅▃▆▄▅▅▇▄▇▅▄▄▅▂▃▅▄█</td></tr></table><br/>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    <br/>Synced <strong style=\"color:#cdcd00\">TL50FilesShuffle10KSteps400_AdamLR1e-4_Base</strong>: <a href=\"https://wandb.ai/amitagni/candlestick-CNN/runs/mkbs1wyl\" target=\"_blank\">https://wandb.ai/amitagni/candlestick-CNN/runs/mkbs1wyl</a><br/>\n                "
     },
     "metadata": {}
    }
   ],
   "source": [
    "run = wandb.init(project=\"candlestick-CNN\", name = 'TL50FilesShuffle10KSteps400_AdamLR1e-4_Base' ,reinit= True)\n",
    "\n",
    "history = model.fit(train\n",
    "                ,epochs=100\n",
    "                ,steps_per_epoch=10240*10/256 #800\n",
    "                ,verbose=1\n",
    "                ,validation_data=test                \n",
    "                ,validation_freq = 5\n",
    "                ,validation_steps = 10\n",
    "                ,callbacks=[WandbCallback()]\n",
    "                )\n",
    "\n",
    "\n",
    "run.finish()\n"
   ]
  },
  {
   "source": [
    "Epoch 10/10\n",
    "800/800 [==============================] - 94s 117ms/step - loss: 0.5294 - precision: 0.6667 - recall: 5.6124e-04 - auc: 0.5652 - val_loss: 0.6202 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5234\n",
    "\n",
    "\n",
    "### Fine tuning "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of layers in the base model:  155\n"
     ]
    }
   ],
   "source": [
    "# unfreeze the base_model and set the bottom layers to be un-trainable\n",
    "\n",
    "base_model.trainable = True\n",
    "\n",
    "# Let's take a look to see how many layers are in the base model\n",
    "print(\"Number of layers in the base model: \", len(base_model.layers))\n",
    "\n",
    "# Fine-tune from this layer onwards\n",
    "fine_tune_at = 100\n",
    "\n",
    "# Freeze all the layers before the `fine_tune_at` layer\n",
    "for layer in base_model.layers[:fine_tune_at]:\n",
    "  layer.trainable =  False\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"functional_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_2 (InputLayer)         [(None, 128, 128, 3)]     0         \n_________________________________________________________________\ntf_op_layer_RealDiv (TensorF [(None, 128, 128, 3)]     0         \n_________________________________________________________________\ntf_op_layer_Sub (TensorFlowO [(None, 128, 128, 3)]     0         \n_________________________________________________________________\nmobilenetv2_1.00_128 (Functi (None, 4, 4, 1280)        2257984   \n_________________________________________________________________\nglobal_average_pooling2d (Gl (None, 1280)              0         \n_________________________________________________________________\ndropout (Dropout)            (None, 1280)              0         \n_________________________________________________________________\ndense (Dense)                (None, 1)                 1281      \n=================================================================\nTotal params: 2,259,265\nTrainable params: 1,863,873\nNon-trainable params: 395,392\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# As you are training a much larger model and want to readapt the pretrained weights, it is important to use a lower learning rate at this stage. Otherwise, your model could overfit very quickly.\n",
    "\n",
    "# model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "#               optimizer = tf.keras.optimizers.RMSprop(lr=base_learning_rate/10),\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=base_learning_rate/100),\n",
    "            #   loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              loss=tf.keras.losses.binary_crossentropy,\n",
    "              metrics=METRICS)\n",
    "\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.10 is available!  To upgrade, please run:\n\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                Tracking run with wandb version 0.10.8<br/>\n                Syncing run <strong style=\"color:#cdcd00\">TL50FilesShuffle100K_AdamLR1e-6_Finetuned</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://wandb.ai/amitagni/candlestick-CNN\" target=\"_blank\">https://wandb.ai/amitagni/candlestick-CNN</a><br/>\n                Run page: <a href=\"https://wandb.ai/amitagni/candlestick-CNN/runs/9yusi0ko\" target=\"_blank\">https://wandb.ai/amitagni/candlestick-CNN/runs/9yusi0ko</a><br/>\n                Run data is saved locally in <code>wandb/run-20201111_113316-9yusi0ko</code><br/><br/>\n            "
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 2/1000\n",
      "400/400 [==============================] - 45s 112ms/step - loss: 0.5639 - precision: 0.2520 - recall: 0.0333 - auc: 0.5110\n",
      "Epoch 3/1000\n",
      "400/400 [==============================] - 45s 113ms/step - loss: 0.5536 - precision: 0.3125 - recall: 6.1828e-04 - auc: 0.5221\n",
      "Epoch 4/1000\n",
      "400/400 [==============================] - 45s 113ms/step - loss: 0.5528 - precision: 0.3333 - recall: 8.2001e-05 - auc: 0.5223\n",
      "Epoch 5/1000\n",
      "400/400 [==============================] - 48s 120ms/step - loss: 0.5492 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5297 - val_loss: 0.5468 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5443\n",
      "Epoch 6/1000\n",
      "400/400 [==============================] - 45s 113ms/step - loss: 0.5522 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5303\n",
      "Epoch 7/1000\n",
      "400/400 [==============================] - 45s 113ms/step - loss: 0.5476 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5328\n",
      "Epoch 8/1000\n",
      "400/400 [==============================] - 45s 112ms/step - loss: 0.5511 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5351\n",
      "Epoch 9/1000\n",
      "400/400 [==============================] - 45s 113ms/step - loss: 0.5455 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5385\n",
      "Epoch 10/1000\n",
      "400/400 [==============================] - 47s 117ms/step - loss: 0.5507 - precision: 1.0000 - recall: 4.0636e-05 - auc: 0.5397 - val_loss: 0.5503 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.4991\n",
      "Epoch 11/1000\n",
      "400/400 [==============================] - 45s 113ms/step - loss: 0.5447 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5398\n",
      "Epoch 12/1000\n",
      "400/400 [==============================] - 45s 113ms/step - loss: 0.5491 - precision: 1.0000 - recall: 4.0670e-05 - auc: 0.5468\n",
      "Epoch 13/1000\n",
      "400/400 [==============================] - 45s 113ms/step - loss: 0.5449 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5415\n",
      "Epoch 14/1000\n",
      "400/400 [==============================] - 45s 113ms/step - loss: 0.5489 - precision: 1.0000 - recall: 4.0780e-05 - auc: 0.5433\n",
      "Epoch 15/1000\n",
      "400/400 [==============================] - 47s 117ms/step - loss: 0.5440 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5498 - val_loss: 0.5572 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5103\n",
      "Epoch 16/1000\n",
      "400/400 [==============================] - 45s 114ms/step - loss: 0.5502 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5484\n",
      "Epoch 17/1000\n",
      " 60/400 [===>..........................] - ETA: 37s - loss: 0.5444 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5442"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-141998ac6c09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m                 \u001b[0;34m,\u001b[0m\u001b[0mvalidation_freq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                 \u001b[0;34m,\u001b[0m\u001b[0mvalidation_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m                 \u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mWandbCallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m                 )\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/wandb/integration/keras/keras.py\u001b[0m in \u001b[0;36mnew_v2\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcbk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcbks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0mset_wandb_attrs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcbk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mold_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0mtraining_arrays\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morig_fit_loop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Continue training the model\n",
    "# If you trained to convergence earlier, this step will improve your accuracy by a few percentage points.\n",
    "\n",
    "# fine_tune_epochs = 10\n",
    "# total_epochs =  initial_epochs + fine_tune_epochs\n",
    "\n",
    "# history_fine = model.fit(train_dataset,\n",
    "#                          epochs=total_epochs,\n",
    "#                          initial_epoch=history.epoch[-1],\n",
    "#                          validation_data=validation_dataset)\n",
    "\n",
    "run = wandb.init(project=\"candlestick-CNN\", name = 'TL50FilesShuffle10KSteps400_AdamLR1e-6_Finetuned' ,reinit= True)\n",
    "\n",
    "history = model.fit(train\n",
    "                ,epochs=800\n",
    "                ,steps_per_epoch=10240*10/256 #800\n",
    "                ,initial_epoch=history.epoch[-1]\n",
    "                ,verbose=1\n",
    "                ,validation_data=test                \n",
    "                ,validation_freq = 5\n",
    "                ,validation_steps = 5\n",
    "                ,callbacks=[WandbCallback()]\n",
    "                )\n",
    "\n",
    "\n",
    "run.finish()\n",
    "\n",
    "                         "
   ]
  },
  {
   "source": [
    "Epoch 20/20\n",
    "800/800 [==============================] - 116s 145ms/step - loss: 0.2530 - precision: 0.8447 - recall: 0.6638 - auc: 0.9469 - val_loss: 0.9897 - val_precision: 0.3161 - val_recall: 0.1632 - val_auc: 0.5052"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}