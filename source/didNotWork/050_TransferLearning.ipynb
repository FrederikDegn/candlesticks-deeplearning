{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.6.9 64-bit",
   "display_name": "Python 3.6.9 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "4cd7ab41f5fca4b9b44701077e38c5ffd31fe66a6cab21e0214b68d958d0e462"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "### Package Setups\n",
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "from tensorflow import keras\n",
    "import time\n",
    "\n",
    "from functions_dataCreation import *\n",
    "from functions_modelArchitectures import *\n",
    "\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "### Data Loading\n",
    "train = createIODataset(20,'../data/Train')\n",
    "test = createIODataset(1,'../data/Test')\n",
    "\n",
    "train = train.repeat(-1)\n",
    "train = train.shuffle(buffer_size=1024*20,reshuffle_each_iteration=True)\n",
    "train = train.batch(256,drop_remainder=True)\n",
    "train = train.prefetch(100)\n",
    "\n",
    "test = test.repeat(-1)\n",
    "test = test.shuffle(buffer_size=1024,reshuffle_each_iteration=True)\n",
    "test = test.batch(1024,drop_remainder=True)\n",
    "test = test.prefetch(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MobileNetV2 expects pixel vaues in [-1,1], but at this point, the pixel values in your images are in [0-255]. \n",
    " \n",
    "preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the base model from the pre-trained model MobileNet V2\n",
    "IMG_SIZE = 128\n",
    "base_model = tf.keras.applications.MobileNetV2(input_shape=(IMG_SIZE,IMG_SIZE,3),include_top=False,weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"mobilenetv2_1.00_128\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_1 (InputLayer)            [(None, 128, 128, 3) 0                                            \n__________________________________________________________________________________________________\nConv1_pad (ZeroPadding2D)       (None, 129, 129, 3)  0           input_1[0][0]                    \n__________________________________________________________________________________________________\nConv1 (Conv2D)                  (None, 64, 64, 32)   864         Conv1_pad[0][0]                  \n__________________________________________________________________________________________________\nbn_Conv1 (BatchNormalization)   (None, 64, 64, 32)   128         Conv1[0][0]                      \n__________________________________________________________________________________________________\nConv1_relu (ReLU)               (None, 64, 64, 32)   0           bn_Conv1[0][0]                   \n__________________________________________________________________________________________________\nexpanded_conv_depthwise (Depthw (None, 64, 64, 32)   288         Conv1_relu[0][0]                 \n__________________________________________________________________________________________________\nexpanded_conv_depthwise_BN (Bat (None, 64, 64, 32)   128         expanded_conv_depthwise[0][0]    \n__________________________________________________________________________________________________\nexpanded_conv_depthwise_relu (R (None, 64, 64, 32)   0           expanded_conv_depthwise_BN[0][0] \n__________________________________________________________________________________________________\nexpanded_conv_project (Conv2D)  (None, 64, 64, 16)   512         expanded_conv_depthwise_relu[0][0\n__________________________________________________________________________________________________\nexpanded_conv_project_BN (Batch (None, 64, 64, 16)   64          expanded_conv_project[0][0]      \n__________________________________________________________________________________________________\nblock_1_expand (Conv2D)         (None, 64, 64, 96)   1536        expanded_conv_project_BN[0][0]   \n__________________________________________________________________________________________________\nblock_1_expand_BN (BatchNormali (None, 64, 64, 96)   384         block_1_expand[0][0]             \n__________________________________________________________________________________________________\nblock_1_expand_relu (ReLU)      (None, 64, 64, 96)   0           block_1_expand_BN[0][0]          \n__________________________________________________________________________________________________\nblock_1_pad (ZeroPadding2D)     (None, 65, 65, 96)   0           block_1_expand_relu[0][0]        \n__________________________________________________________________________________________________\nblock_1_depthwise (DepthwiseCon (None, 32, 32, 96)   864         block_1_pad[0][0]                \n__________________________________________________________________________________________________\nblock_1_depthwise_BN (BatchNorm (None, 32, 32, 96)   384         block_1_depthwise[0][0]          \n__________________________________________________________________________________________________\nblock_1_depthwise_relu (ReLU)   (None, 32, 32, 96)   0           block_1_depthwise_BN[0][0]       \n__________________________________________________________________________________________________\nblock_1_project (Conv2D)        (None, 32, 32, 24)   2304        block_1_depthwise_relu[0][0]     \n__________________________________________________________________________________________________\nblock_1_project_BN (BatchNormal (None, 32, 32, 24)   96          block_1_project[0][0]            \n__________________________________________________________________________________________________\nblock_2_expand (Conv2D)         (None, 32, 32, 144)  3456        block_1_project_BN[0][0]         \n__________________________________________________________________________________________________\nblock_2_expand_BN (BatchNormali (None, 32, 32, 144)  576         block_2_expand[0][0]             \n__________________________________________________________________________________________________\nblock_2_expand_relu (ReLU)      (None, 32, 32, 144)  0           block_2_expand_BN[0][0]          \n__________________________________________________________________________________________________\nblock_2_depthwise (DepthwiseCon (None, 32, 32, 144)  1296        block_2_expand_relu[0][0]        \n__________________________________________________________________________________________________\nblock_2_depthwise_BN (BatchNorm (None, 32, 32, 144)  576         block_2_depthwise[0][0]          \n__________________________________________________________________________________________________\nblock_2_depthwise_relu (ReLU)   (None, 32, 32, 144)  0           block_2_depthwise_BN[0][0]       \n__________________________________________________________________________________________________\nblock_2_project (Conv2D)        (None, 32, 32, 24)   3456        block_2_depthwise_relu[0][0]     \n__________________________________________________________________________________________________\nblock_2_project_BN (BatchNormal (None, 32, 32, 24)   96          block_2_project[0][0]            \n__________________________________________________________________________________________________\nblock_2_add (Add)               (None, 32, 32, 24)   0           block_1_project_BN[0][0]         \n                                                                 block_2_project_BN[0][0]         \n__________________________________________________________________________________________________\nblock_3_expand (Conv2D)         (None, 32, 32, 144)  3456        block_2_add[0][0]                \n__________________________________________________________________________________________________\nblock_3_expand_BN (BatchNormali (None, 32, 32, 144)  576         block_3_expand[0][0]             \n__________________________________________________________________________________________________\nblock_3_expand_relu (ReLU)      (None, 32, 32, 144)  0           block_3_expand_BN[0][0]          \n__________________________________________________________________________________________________\nblock_3_pad (ZeroPadding2D)     (None, 33, 33, 144)  0           block_3_expand_relu[0][0]        \n__________________________________________________________________________________________________\nblock_3_depthwise (DepthwiseCon (None, 16, 16, 144)  1296        block_3_pad[0][0]                \n__________________________________________________________________________________________________\nblock_3_depthwise_BN (BatchNorm (None, 16, 16, 144)  576         block_3_depthwise[0][0]          \n__________________________________________________________________________________________________\nblock_3_depthwise_relu (ReLU)   (None, 16, 16, 144)  0           block_3_depthwise_BN[0][0]       \n__________________________________________________________________________________________________\nblock_3_project (Conv2D)        (None, 16, 16, 32)   4608        block_3_depthwise_relu[0][0]     \n__________________________________________________________________________________________________\nblock_3_project_BN (BatchNormal (None, 16, 16, 32)   128         block_3_project[0][0]            \n__________________________________________________________________________________________________\nblock_4_expand (Conv2D)         (None, 16, 16, 192)  6144        block_3_project_BN[0][0]         \n__________________________________________________________________________________________________\nblock_4_expand_BN (BatchNormali (None, 16, 16, 192)  768         block_4_expand[0][0]             \n__________________________________________________________________________________________________\nblock_4_expand_relu (ReLU)      (None, 16, 16, 192)  0           block_4_expand_BN[0][0]          \n__________________________________________________________________________________________________\nblock_4_depthwise (DepthwiseCon (None, 16, 16, 192)  1728        block_4_expand_relu[0][0]        \n__________________________________________________________________________________________________\nblock_4_depthwise_BN (BatchNorm (None, 16, 16, 192)  768         block_4_depthwise[0][0]          \n__________________________________________________________________________________________________\nblock_4_depthwise_relu (ReLU)   (None, 16, 16, 192)  0           block_4_depthwise_BN[0][0]       \n__________________________________________________________________________________________________\nblock_4_project (Conv2D)        (None, 16, 16, 32)   6144        block_4_depthwise_relu[0][0]     \n__________________________________________________________________________________________________\nblock_4_project_BN (BatchNormal (None, 16, 16, 32)   128         block_4_project[0][0]            \n__________________________________________________________________________________________________\nblock_4_add (Add)               (None, 16, 16, 32)   0           block_3_project_BN[0][0]         \n                                                                 block_4_project_BN[0][0]         \n__________________________________________________________________________________________________\nblock_5_expand (Conv2D)         (None, 16, 16, 192)  6144        block_4_add[0][0]                \n__________________________________________________________________________________________________\nblock_5_expand_BN (BatchNormali (None, 16, 16, 192)  768         block_5_expand[0][0]             \n__________________________________________________________________________________________________\nblock_5_expand_relu (ReLU)      (None, 16, 16, 192)  0           block_5_expand_BN[0][0]          \n__________________________________________________________________________________________________\nblock_5_depthwise (DepthwiseCon (None, 16, 16, 192)  1728        block_5_expand_relu[0][0]        \n__________________________________________________________________________________________________\nblock_5_depthwise_BN (BatchNorm (None, 16, 16, 192)  768         block_5_depthwise[0][0]          \n__________________________________________________________________________________________________\nblock_5_depthwise_relu (ReLU)   (None, 16, 16, 192)  0           block_5_depthwise_BN[0][0]       \n__________________________________________________________________________________________________\nblock_5_project (Conv2D)        (None, 16, 16, 32)   6144        block_5_depthwise_relu[0][0]     \n__________________________________________________________________________________________________\nblock_5_project_BN (BatchNormal (None, 16, 16, 32)   128         block_5_project[0][0]            \n__________________________________________________________________________________________________\nblock_5_add (Add)               (None, 16, 16, 32)   0           block_4_add[0][0]                \n                                                                 block_5_project_BN[0][0]         \n__________________________________________________________________________________________________\nblock_6_expand (Conv2D)         (None, 16, 16, 192)  6144        block_5_add[0][0]                \n__________________________________________________________________________________________________\nblock_6_expand_BN (BatchNormali (None, 16, 16, 192)  768         block_6_expand[0][0]             \n__________________________________________________________________________________________________\nblock_6_expand_relu (ReLU)      (None, 16, 16, 192)  0           block_6_expand_BN[0][0]          \n__________________________________________________________________________________________________\nblock_6_pad (ZeroPadding2D)     (None, 17, 17, 192)  0           block_6_expand_relu[0][0]        \n__________________________________________________________________________________________________\nblock_6_depthwise (DepthwiseCon (None, 8, 8, 192)    1728        block_6_pad[0][0]                \n__________________________________________________________________________________________________\nblock_6_depthwise_BN (BatchNorm (None, 8, 8, 192)    768         block_6_depthwise[0][0]          \n__________________________________________________________________________________________________\nblock_6_depthwise_relu (ReLU)   (None, 8, 8, 192)    0           block_6_depthwise_BN[0][0]       \n__________________________________________________________________________________________________\nblock_6_project (Conv2D)        (None, 8, 8, 64)     12288       block_6_depthwise_relu[0][0]     \n__________________________________________________________________________________________________\nblock_6_project_BN (BatchNormal (None, 8, 8, 64)     256         block_6_project[0][0]            \n__________________________________________________________________________________________________\nblock_7_expand (Conv2D)         (None, 8, 8, 384)    24576       block_6_project_BN[0][0]         \n__________________________________________________________________________________________________\nblock_7_expand_BN (BatchNormali (None, 8, 8, 384)    1536        block_7_expand[0][0]             \n__________________________________________________________________________________________________\nblock_7_expand_relu (ReLU)      (None, 8, 8, 384)    0           block_7_expand_BN[0][0]          \n__________________________________________________________________________________________________\nblock_7_depthwise (DepthwiseCon (None, 8, 8, 384)    3456        block_7_expand_relu[0][0]        \n__________________________________________________________________________________________________\nblock_7_depthwise_BN (BatchNorm (None, 8, 8, 384)    1536        block_7_depthwise[0][0]          \n__________________________________________________________________________________________________\nblock_7_depthwise_relu (ReLU)   (None, 8, 8, 384)    0           block_7_depthwise_BN[0][0]       \n__________________________________________________________________________________________________\nblock_7_project (Conv2D)        (None, 8, 8, 64)     24576       block_7_depthwise_relu[0][0]     \n__________________________________________________________________________________________________\nblock_7_project_BN (BatchNormal (None, 8, 8, 64)     256         block_7_project[0][0]            \n__________________________________________________________________________________________________\nblock_7_add (Add)               (None, 8, 8, 64)     0           block_6_project_BN[0][0]         \n                                                                 block_7_project_BN[0][0]         \n__________________________________________________________________________________________________\nblock_8_expand (Conv2D)         (None, 8, 8, 384)    24576       block_7_add[0][0]                \n__________________________________________________________________________________________________\nblock_8_expand_BN (BatchNormali (None, 8, 8, 384)    1536        block_8_expand[0][0]             \n__________________________________________________________________________________________________\nblock_8_expand_relu (ReLU)      (None, 8, 8, 384)    0           block_8_expand_BN[0][0]          \n__________________________________________________________________________________________________\nblock_8_depthwise (DepthwiseCon (None, 8, 8, 384)    3456        block_8_expand_relu[0][0]        \n__________________________________________________________________________________________________\nblock_8_depthwise_BN (BatchNorm (None, 8, 8, 384)    1536        block_8_depthwise[0][0]          \n__________________________________________________________________________________________________\nblock_8_depthwise_relu (ReLU)   (None, 8, 8, 384)    0           block_8_depthwise_BN[0][0]       \n__________________________________________________________________________________________________\nblock_8_project (Conv2D)        (None, 8, 8, 64)     24576       block_8_depthwise_relu[0][0]     \n__________________________________________________________________________________________________\nblock_8_project_BN (BatchNormal (None, 8, 8, 64)     256         block_8_project[0][0]            \n__________________________________________________________________________________________________\nblock_8_add (Add)               (None, 8, 8, 64)     0           block_7_add[0][0]                \n                                                                 block_8_project_BN[0][0]         \n__________________________________________________________________________________________________\nblock_9_expand (Conv2D)         (None, 8, 8, 384)    24576       block_8_add[0][0]                \n__________________________________________________________________________________________________\nblock_9_expand_BN (BatchNormali (None, 8, 8, 384)    1536        block_9_expand[0][0]             \n__________________________________________________________________________________________________\nblock_9_expand_relu (ReLU)      (None, 8, 8, 384)    0           block_9_expand_BN[0][0]          \n__________________________________________________________________________________________________\nblock_9_depthwise (DepthwiseCon (None, 8, 8, 384)    3456        block_9_expand_relu[0][0]        \n__________________________________________________________________________________________________\nblock_9_depthwise_BN (BatchNorm (None, 8, 8, 384)    1536        block_9_depthwise[0][0]          \n__________________________________________________________________________________________________\nblock_9_depthwise_relu (ReLU)   (None, 8, 8, 384)    0           block_9_depthwise_BN[0][0]       \n__________________________________________________________________________________________________\nblock_9_project (Conv2D)        (None, 8, 8, 64)     24576       block_9_depthwise_relu[0][0]     \n__________________________________________________________________________________________________\nblock_9_project_BN (BatchNormal (None, 8, 8, 64)     256         block_9_project[0][0]            \n__________________________________________________________________________________________________\nblock_9_add (Add)               (None, 8, 8, 64)     0           block_8_add[0][0]                \n                                                                 block_9_project_BN[0][0]         \n__________________________________________________________________________________________________\nblock_10_expand (Conv2D)        (None, 8, 8, 384)    24576       block_9_add[0][0]                \n__________________________________________________________________________________________________\nblock_10_expand_BN (BatchNormal (None, 8, 8, 384)    1536        block_10_expand[0][0]            \n__________________________________________________________________________________________________\nblock_10_expand_relu (ReLU)     (None, 8, 8, 384)    0           block_10_expand_BN[0][0]         \n__________________________________________________________________________________________________\nblock_10_depthwise (DepthwiseCo (None, 8, 8, 384)    3456        block_10_expand_relu[0][0]       \n__________________________________________________________________________________________________\nblock_10_depthwise_BN (BatchNor (None, 8, 8, 384)    1536        block_10_depthwise[0][0]         \n__________________________________________________________________________________________________\nblock_10_depthwise_relu (ReLU)  (None, 8, 8, 384)    0           block_10_depthwise_BN[0][0]      \n__________________________________________________________________________________________________\nblock_10_project (Conv2D)       (None, 8, 8, 96)     36864       block_10_depthwise_relu[0][0]    \n__________________________________________________________________________________________________\nblock_10_project_BN (BatchNorma (None, 8, 8, 96)     384         block_10_project[0][0]           \n__________________________________________________________________________________________________\nblock_11_expand (Conv2D)        (None, 8, 8, 576)    55296       block_10_project_BN[0][0]        \n__________________________________________________________________________________________________\nblock_11_expand_BN (BatchNormal (None, 8, 8, 576)    2304        block_11_expand[0][0]            \n__________________________________________________________________________________________________\nblock_11_expand_relu (ReLU)     (None, 8, 8, 576)    0           block_11_expand_BN[0][0]         \n__________________________________________________________________________________________________\nblock_11_depthwise (DepthwiseCo (None, 8, 8, 576)    5184        block_11_expand_relu[0][0]       \n__________________________________________________________________________________________________\nblock_11_depthwise_BN (BatchNor (None, 8, 8, 576)    2304        block_11_depthwise[0][0]         \n__________________________________________________________________________________________________\nblock_11_depthwise_relu (ReLU)  (None, 8, 8, 576)    0           block_11_depthwise_BN[0][0]      \n__________________________________________________________________________________________________\nblock_11_project (Conv2D)       (None, 8, 8, 96)     55296       block_11_depthwise_relu[0][0]    \n__________________________________________________________________________________________________\nblock_11_project_BN (BatchNorma (None, 8, 8, 96)     384         block_11_project[0][0]           \n__________________________________________________________________________________________________\nblock_11_add (Add)              (None, 8, 8, 96)     0           block_10_project_BN[0][0]        \n                                                                 block_11_project_BN[0][0]        \n__________________________________________________________________________________________________\nblock_12_expand (Conv2D)        (None, 8, 8, 576)    55296       block_11_add[0][0]               \n__________________________________________________________________________________________________\nblock_12_expand_BN (BatchNormal (None, 8, 8, 576)    2304        block_12_expand[0][0]            \n__________________________________________________________________________________________________\nblock_12_expand_relu (ReLU)     (None, 8, 8, 576)    0           block_12_expand_BN[0][0]         \n__________________________________________________________________________________________________\nblock_12_depthwise (DepthwiseCo (None, 8, 8, 576)    5184        block_12_expand_relu[0][0]       \n__________________________________________________________________________________________________\nblock_12_depthwise_BN (BatchNor (None, 8, 8, 576)    2304        block_12_depthwise[0][0]         \n__________________________________________________________________________________________________\nblock_12_depthwise_relu (ReLU)  (None, 8, 8, 576)    0           block_12_depthwise_BN[0][0]      \n__________________________________________________________________________________________________\nblock_12_project (Conv2D)       (None, 8, 8, 96)     55296       block_12_depthwise_relu[0][0]    \n__________________________________________________________________________________________________\nblock_12_project_BN (BatchNorma (None, 8, 8, 96)     384         block_12_project[0][0]           \n__________________________________________________________________________________________________\nblock_12_add (Add)              (None, 8, 8, 96)     0           block_11_add[0][0]               \n                                                                 block_12_project_BN[0][0]        \n__________________________________________________________________________________________________\nblock_13_expand (Conv2D)        (None, 8, 8, 576)    55296       block_12_add[0][0]               \n__________________________________________________________________________________________________\nblock_13_expand_BN (BatchNormal (None, 8, 8, 576)    2304        block_13_expand[0][0]            \n__________________________________________________________________________________________________\nblock_13_expand_relu (ReLU)     (None, 8, 8, 576)    0           block_13_expand_BN[0][0]         \n__________________________________________________________________________________________________\nblock_13_pad (ZeroPadding2D)    (None, 9, 9, 576)    0           block_13_expand_relu[0][0]       \n__________________________________________________________________________________________________\nblock_13_depthwise (DepthwiseCo (None, 4, 4, 576)    5184        block_13_pad[0][0]               \n__________________________________________________________________________________________________\nblock_13_depthwise_BN (BatchNor (None, 4, 4, 576)    2304        block_13_depthwise[0][0]         \n__________________________________________________________________________________________________\nblock_13_depthwise_relu (ReLU)  (None, 4, 4, 576)    0           block_13_depthwise_BN[0][0]      \n__________________________________________________________________________________________________\nblock_13_project (Conv2D)       (None, 4, 4, 160)    92160       block_13_depthwise_relu[0][0]    \n__________________________________________________________________________________________________\nblock_13_project_BN (BatchNorma (None, 4, 4, 160)    640         block_13_project[0][0]           \n__________________________________________________________________________________________________\nblock_14_expand (Conv2D)        (None, 4, 4, 960)    153600      block_13_project_BN[0][0]        \n__________________________________________________________________________________________________\nblock_14_expand_BN (BatchNormal (None, 4, 4, 960)    3840        block_14_expand[0][0]            \n__________________________________________________________________________________________________\nblock_14_expand_relu (ReLU)     (None, 4, 4, 960)    0           block_14_expand_BN[0][0]         \n__________________________________________________________________________________________________\nblock_14_depthwise (DepthwiseCo (None, 4, 4, 960)    8640        block_14_expand_relu[0][0]       \n__________________________________________________________________________________________________\nblock_14_depthwise_BN (BatchNor (None, 4, 4, 960)    3840        block_14_depthwise[0][0]         \n__________________________________________________________________________________________________\nblock_14_depthwise_relu (ReLU)  (None, 4, 4, 960)    0           block_14_depthwise_BN[0][0]      \n__________________________________________________________________________________________________\nblock_14_project (Conv2D)       (None, 4, 4, 160)    153600      block_14_depthwise_relu[0][0]    \n__________________________________________________________________________________________________\nblock_14_project_BN (BatchNorma (None, 4, 4, 160)    640         block_14_project[0][0]           \n__________________________________________________________________________________________________\nblock_14_add (Add)              (None, 4, 4, 160)    0           block_13_project_BN[0][0]        \n                                                                 block_14_project_BN[0][0]        \n__________________________________________________________________________________________________\nblock_15_expand (Conv2D)        (None, 4, 4, 960)    153600      block_14_add[0][0]               \n__________________________________________________________________________________________________\nblock_15_expand_BN (BatchNormal (None, 4, 4, 960)    3840        block_15_expand[0][0]            \n__________________________________________________________________________________________________\nblock_15_expand_relu (ReLU)     (None, 4, 4, 960)    0           block_15_expand_BN[0][0]         \n__________________________________________________________________________________________________\nblock_15_depthwise (DepthwiseCo (None, 4, 4, 960)    8640        block_15_expand_relu[0][0]       \n__________________________________________________________________________________________________\nblock_15_depthwise_BN (BatchNor (None, 4, 4, 960)    3840        block_15_depthwise[0][0]         \n__________________________________________________________________________________________________\nblock_15_depthwise_relu (ReLU)  (None, 4, 4, 960)    0           block_15_depthwise_BN[0][0]      \n__________________________________________________________________________________________________\nblock_15_project (Conv2D)       (None, 4, 4, 160)    153600      block_15_depthwise_relu[0][0]    \n__________________________________________________________________________________________________\nblock_15_project_BN (BatchNorma (None, 4, 4, 160)    640         block_15_project[0][0]           \n__________________________________________________________________________________________________\nblock_15_add (Add)              (None, 4, 4, 160)    0           block_14_add[0][0]               \n                                                                 block_15_project_BN[0][0]        \n__________________________________________________________________________________________________\nblock_16_expand (Conv2D)        (None, 4, 4, 960)    153600      block_15_add[0][0]               \n__________________________________________________________________________________________________\nblock_16_expand_BN (BatchNormal (None, 4, 4, 960)    3840        block_16_expand[0][0]            \n__________________________________________________________________________________________________\nblock_16_expand_relu (ReLU)     (None, 4, 4, 960)    0           block_16_expand_BN[0][0]         \n__________________________________________________________________________________________________\nblock_16_depthwise (DepthwiseCo (None, 4, 4, 960)    8640        block_16_expand_relu[0][0]       \n__________________________________________________________________________________________________\nblock_16_depthwise_BN (BatchNor (None, 4, 4, 960)    3840        block_16_depthwise[0][0]         \n__________________________________________________________________________________________________\nblock_16_depthwise_relu (ReLU)  (None, 4, 4, 960)    0           block_16_depthwise_BN[0][0]      \n__________________________________________________________________________________________________\nblock_16_project (Conv2D)       (None, 4, 4, 320)    307200      block_16_depthwise_relu[0][0]    \n__________________________________________________________________________________________________\nblock_16_project_BN (BatchNorma (None, 4, 4, 320)    1280        block_16_project[0][0]           \n__________________________________________________________________________________________________\nConv_1 (Conv2D)                 (None, 4, 4, 1280)   409600      block_16_project_BN[0][0]        \n__________________________________________________________________________________________________\nConv_1_bn (BatchNormalization)  (None, 4, 4, 1280)   5120        Conv_1[0][0]                     \n__________________________________________________________________________________________________\nout_relu (ReLU)                 (None, 4, 4, 1280)   0           Conv_1_bn[0][0]                  \n==================================================================================================\nTotal params: 2,257,984\nTrainable params: 0\nNon-trainable params: 2,257,984\n__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Freezing (by setting layer.trainable = False) prevents the weights in a given layer from being updated during training.\n",
    "base_model.trainable = False\n",
    "base_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(256, 4, 4, 1280)\n"
     ]
    }
   ],
   "source": [
    "#This feature extractor converts each 128x128x3 image into a 5x5x1280 block of features. Let's see what it does to an example batch of images. (Batch size being 256)\n",
    "\n",
    "image_batch, label_batch = next(iter(train))\n",
    "feature_batch = base_model(image_batch)\n",
    "print(feature_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(256, 1280)\n"
     ]
    }
   ],
   "source": [
    "# Add a classification head\n",
    "# To generate predictions from the block of features, average over the spatial 5x5 spatial locations, using a tf.keras.layers.GlobalAveragePooling2D layer to convert the features to a single 1280-element vector per image.\n",
    "\n",
    "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "feature_batch_average = global_average_layer(feature_batch)\n",
    "print(feature_batch_average.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(256, 1)\n"
     ]
    }
   ],
   "source": [
    "# Apply a tf.keras.layers.Dense layer to convert these features into a single prediction per image. You don't need an activation function here because this prediction will be treated as a logit, or a raw prediction value. Positive numbers predict class 1, negative numbers predict class 0.\n",
    "\n",
    "# prediction_layer = tf.keras.layers.Dense(1)\n",
    "prediction_layer = tf.keras.layers.Dense(1,activation='sigmoid')\n",
    "prediction_batch = prediction_layer(feature_batch_average)\n",
    "print(prediction_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a model by chaining together the rescaling, base_model and feature extractor layers using the Keras Functional API. As previously mentioned, use training=False as our model contains a BatchNormalization layer.\n",
    "\n",
    "inputs = tf.keras.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "x = preprocess_input(inputs)\n",
    "x = base_model(x, training=False)\n",
    "x = global_average_layer(x)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "outputs = prediction_layer(x)\n",
    "model = tf.keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"functional_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_2 (InputLayer)         [(None, 128, 128, 3)]     0         \n_________________________________________________________________\ntf_op_layer_RealDiv (TensorF [(None, 128, 128, 3)]     0         \n_________________________________________________________________\ntf_op_layer_Sub (TensorFlowO [(None, 128, 128, 3)]     0         \n_________________________________________________________________\nmobilenetv2_1.00_128 (Functi (None, 4, 4, 1280)        2257984   \n_________________________________________________________________\nglobal_average_pooling2d (Gl (None, 1280)              0         \n_________________________________________________________________\ndropout (Dropout)            (None, 1280)              0         \n_________________________________________________________________\ndense (Dense)                (None, 1)                 1281      \n=================================================================\nTotal params: 2,259,265\nTrainable params: 1,281\nNon-trainable params: 2,257,984\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Compile the model before training it. Since there are two classes, use a binary cross-entropy loss with from_logits=True since the model provides a linear output.\n",
    "\n",
    "METRICS = [keras.metrics.Precision(name='precision'),keras.metrics.Recall(name='recall'),keras.metrics.AUC(name='auc'),]\n",
    "\n",
    "base_learning_rate = 0.0001\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=base_learning_rate),\n",
    "            #   loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              loss=tf.keras.losses.binary_crossentropy,\n",
    "              metrics=METRICS)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "# The 2.5M parameters in MobileNet are frozen, but there are 1.2K trainable parameters in the Dense layer. These are divided between two tf.Variable objects, the weights and biases.\n",
    "\n",
    "len(model.trainable_variables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Failed to query for notebook name, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mamitagni\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.10 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                Tracking run with wandb version 0.10.8<br/>\n                Syncing run <strong style=\"color:#cdcd00\">TLShallow20FilesShuffle20K_AdamLR1e-4</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://wandb.ai/amitagni/candlestick-CNN\" target=\"_blank\">https://wandb.ai/amitagni/candlestick-CNN</a><br/>\n                Run page: <a href=\"https://wandb.ai/amitagni/candlestick-CNN/runs/1m9c1m49\" target=\"_blank\">https://wandb.ai/amitagni/candlestick-CNN/runs/1m9c1m49</a><br/>\n                Run data is saved locally in <code>wandb/run-20201111_040807-1m9c1m49</code><br/><br/>\n            "
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/1000\n",
      "800/800 [==============================] - 63s 79ms/step - loss: 0.5822 - precision: 0.2437 - recall: 0.0473 - auc: 0.5138\n",
      "Epoch 2/1000\n",
      "800/800 [==============================] - 64s 79ms/step - loss: 0.5538 - precision: 0.2554 - recall: 0.0169 - auc: 0.5262\n",
      "Epoch 3/1000\n",
      "800/800 [==============================] - 63s 79ms/step - loss: 0.5485 - precision: 0.2639 - recall: 0.0065 - auc: 0.5314\n",
      "Epoch 4/1000\n",
      "800/800 [==============================] - 63s 79ms/step - loss: 0.5370 - precision: 0.3120 - recall: 0.0025 - auc: 0.5424\n",
      "Epoch 5/1000\n",
      "800/800 [==============================] - 64s 79ms/step - loss: 0.5377 - precision: 0.3289 - recall: 0.0011 - auc: 0.5448\n",
      "Epoch 6/1000\n",
      "800/800 [==============================] - 63s 79ms/step - loss: 0.5312 - precision: 0.5139 - recall: 8.0458e-04 - auc: 0.5535\n",
      "Epoch 7/1000\n",
      "800/800 [==============================] - 63s 79ms/step - loss: 0.5333 - precision: 0.4894 - recall: 4.9494e-04 - auc: 0.5540\n",
      "Epoch 8/1000\n",
      "800/800 [==============================] - 63s 79ms/step - loss: 0.5296 - precision: 0.6047 - recall: 5.6228e-04 - auc: 0.5639\n",
      "Epoch 9/1000\n",
      "800/800 [==============================] - 63s 79ms/step - loss: 0.5289 - precision: 0.7083 - recall: 3.6890e-04 - auc: 0.5618\n",
      "Epoch 10/1000\n",
      "800/800 [==============================] - 93s 117ms/step - loss: 0.5294 - precision: 0.8108 - recall: 6.4658e-04 - auc: 0.5688 - val_loss: 0.6267 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5274\n",
      "Epoch 11/1000\n",
      "800/800 [==============================] - 64s 79ms/step - loss: 0.5268 - precision: 0.7073 - recall: 6.3059e-04 - auc: 0.5685\n",
      "Epoch 12/1000\n",
      "800/800 [==============================] - 64s 80ms/step - loss: 0.5299 - precision: 0.7736 - recall: 8.8034e-04 - auc: 0.5702\n",
      "Epoch 13/1000\n",
      "800/800 [==============================] - 63s 79ms/step - loss: 0.5249 - precision: 0.6724 - recall: 8.5151e-04 - auc: 0.5726\n",
      "Epoch 14/1000\n",
      "800/800 [==============================] - 64s 79ms/step - loss: 0.5301 - precision: 0.7308 - recall: 8.1454e-04 - auc: 0.5708\n",
      "Epoch 15/1000\n",
      "800/800 [==============================] - 64s 79ms/step - loss: 0.5241 - precision: 0.7945 - recall: 0.0013 - auc: 0.5757\n",
      "Epoch 16/1000\n",
      "800/800 [==============================] - 64s 79ms/step - loss: 0.5300 - precision: 0.7681 - recall: 0.0011 - auc: 0.5738\n",
      "Epoch 17/1000\n",
      "800/800 [==============================] - 64s 79ms/step - loss: 0.5243 - precision: 0.7612 - recall: 0.0011 - auc: 0.5769\n",
      "Epoch 18/1000\n",
      "800/800 [==============================] - 63s 79ms/step - loss: 0.5291 - precision: 0.6800 - recall: 0.0011 - auc: 0.5739\n",
      "Epoch 19/1000\n",
      "800/800 [==============================] - 64s 79ms/step - loss: 0.5253 - precision: 0.7111 - recall: 0.0014 - auc: 0.5782\n",
      "Epoch 20/1000\n",
      "800/800 [==============================] - 90s 113ms/step - loss: 0.5280 - precision: 0.7571 - recall: 0.0011 - auc: 0.5762 - val_loss: 0.6323 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5248\n",
      "Epoch 21/1000\n",
      "800/800 [==============================] - 64s 79ms/step - loss: 0.5251 - precision: 0.7865 - recall: 0.0015 - auc: 0.5785\n",
      "Epoch 22/1000\n",
      "800/800 [==============================] - 64s 79ms/step - loss: 0.5278 - precision: 0.7500 - recall: 0.0011 - auc: 0.5763\n",
      "Epoch 23/1000\n",
      "800/800 [==============================] - 64s 80ms/step - loss: 0.5266 - precision: 0.7778 - recall: 0.0015 - auc: 0.5771\n",
      "Epoch 24/1000\n",
      "800/800 [==============================] - 64s 80ms/step - loss: 0.5249 - precision: 0.7045 - recall: 0.0013 - auc: 0.5785\n",
      "Epoch 25/1000\n",
      "800/800 [==============================] - 64s 79ms/step - loss: 0.5275 - precision: 0.7320 - recall: 0.0015 - auc: 0.5788\n",
      "Epoch 26/1000\n",
      "800/800 [==============================] - 64s 79ms/step - loss: 0.5242 - precision: 0.7432 - recall: 0.0012 - auc: 0.5798\n",
      "Epoch 27/1000\n",
      "800/800 [==============================] - 64s 80ms/step - loss: 0.5283 - precision: 0.7901 - recall: 0.0014 - auc: 0.5787\n",
      "Epoch 28/1000\n",
      "800/800 [==============================] - 64s 79ms/step - loss: 0.5240 - precision: 0.7436 - recall: 0.0019 - auc: 0.5789\n",
      "Epoch 29/1000\n",
      "800/800 [==============================] - 64s 79ms/step - loss: 0.5281 - precision: 0.7742 - recall: 0.0015 - auc: 0.5804\n",
      "Epoch 30/1000\n",
      "800/800 [==============================] - 91s 113ms/step - loss: 0.5239 - precision: 0.7895 - recall: 0.0016 - auc: 0.5790 - val_loss: 0.6245 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5305\n",
      "Epoch 31/1000\n",
      "800/800 [==============================] - 63s 79ms/step - loss: 0.5287 - precision: 0.7119 - recall: 0.0018 - auc: 0.5791\n",
      "Epoch 32/1000\n",
      "800/800 [==============================] - 64s 80ms/step - loss: 0.5234 - precision: 0.7544 - recall: 0.0019 - auc: 0.5829\n",
      "Epoch 33/1000\n",
      "800/800 [==============================] - 64s 80ms/step - loss: 0.5287 - precision: 0.6792 - recall: 0.0015 - auc: 0.5768\n",
      "Epoch 34/1000\n",
      "800/800 [==============================] - 64s 79ms/step - loss: 0.5245 - precision: 0.7119 - recall: 0.0018 - auc: 0.5825\n",
      "Epoch 35/1000\n",
      "800/800 [==============================] - 64s 80ms/step - loss: 0.5270 - precision: 0.6695 - recall: 0.0017 - auc: 0.5769\n",
      "Epoch 36/1000\n",
      "800/800 [==============================] - 63s 79ms/step - loss: 0.5257 - precision: 0.7154 - recall: 0.0019 - auc: 0.5816\n",
      "Epoch 37/1000\n",
      "800/800 [==============================] - 64s 79ms/step - loss: 0.5251 - precision: 0.6777 - recall: 0.0018 - auc: 0.5812\n",
      "Epoch 38/1000\n",
      "800/800 [==============================] - 64s 79ms/step - loss: 0.5266 - precision: 0.7500 - recall: 0.0020 - auc: 0.5811\n",
      "Epoch 39/1000\n",
      "800/800 [==============================] - 64s 79ms/step - loss: 0.5240 - precision: 0.7107 - recall: 0.0019 - auc: 0.5809\n",
      "Epoch 40/1000\n",
      "800/800 [==============================] - 90s 113ms/step - loss: 0.5280 - precision: 0.7323 - recall: 0.0020 - auc: 0.5800 - val_loss: 0.6305 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5264\n",
      "Epoch 41/1000\n",
      "800/800 [==============================] - 64s 79ms/step - loss: 0.5238 - precision: 0.7589 - recall: 0.0019 - auc: 0.5816\n",
      "Epoch 42/1000\n",
      "800/800 [==============================] - 64s 79ms/step - loss: 0.5280 - precision: 0.6911 - recall: 0.0018 - auc: 0.5787\n",
      "Epoch 43/1000\n",
      "800/800 [==============================] - 64s 80ms/step - loss: 0.5235 - precision: 0.7462 - recall: 0.0021 - auc: 0.5825\n",
      "Epoch 44/1000\n",
      "800/800 [==============================] - 64s 80ms/step - loss: 0.5283 - precision: 0.6880 - recall: 0.0018 - auc: 0.5805\n",
      "Epoch 45/1000\n",
      "800/800 [==============================] - 64s 79ms/step - loss: 0.5238 - precision: 0.7466 - recall: 0.0024 - auc: 0.5824\n",
      "Epoch 46/1000\n",
      "800/800 [==============================] - 64s 80ms/step - loss: 0.5277 - precision: 0.6812 - recall: 0.0020 - auc: 0.5805\n",
      "Epoch 47/1000\n",
      "800/800 [==============================] - 63s 79ms/step - loss: 0.5239 - precision: 0.7163 - recall: 0.0022 - auc: 0.5830\n",
      "Epoch 48/1000\n",
      "800/800 [==============================] - 63s 78ms/step - loss: 0.5278 - precision: 0.6567 - recall: 0.0019 - auc: 0.5785\n",
      "Epoch 49/1000\n",
      "800/800 [==============================] - 63s 79ms/step - loss: 0.5244 - precision: 0.7536 - recall: 0.0023 - auc: 0.5827\n",
      "Epoch 50/1000\n",
      "800/800 [==============================] - 89s 111ms/step - loss: 0.5257 - precision: 0.6842 - recall: 0.0020 - auc: 0.5808 - val_loss: 0.6391 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5244\n",
      "Epoch 51/1000\n",
      "800/800 [==============================] - 63s 79ms/step - loss: 0.5264 - precision: 0.7007 - recall: 0.0022 - auc: 0.5828\n",
      "Epoch 52/1000\n",
      "800/800 [==============================] - 63s 79ms/step - loss: 0.5238 - precision: 0.7500 - recall: 0.0023 - auc: 0.5829\n",
      "Epoch 53/1000\n",
      "800/800 [==============================] - 63s 79ms/step - loss: 0.5282 - precision: 0.6642 - recall: 0.0020 - auc: 0.5802\n",
      "Epoch 54/1000\n",
      "800/800 [==============================] - 63s 79ms/step - loss: 0.5228 - precision: 0.7078 - recall: 0.0024 - auc: 0.5818\n",
      "Epoch 55/1000\n",
      "800/800 [==============================] - 63s 79ms/step - loss: 0.5287 - precision: 0.7538 - recall: 0.0021 - auc: 0.5791\n",
      "Epoch 56/1000\n",
      "800/800 [==============================] - 63s 79ms/step - loss: 0.5228 - precision: 0.7394 - recall: 0.0023 - auc: 0.5808\n",
      "Epoch 57/1000\n",
      "800/800 [==============================] - 63s 79ms/step - loss: 0.5291 - precision: 0.6957 - recall: 0.0021 - auc: 0.5804\n",
      "Epoch 58/1000\n",
      "800/800 [==============================] - 63s 79ms/step - loss: 0.5232 - precision: 0.6369 - recall: 0.0023 - auc: 0.5825\n",
      "Epoch 59/1000\n",
      "800/800 [==============================] - 63s 79ms/step - loss: 0.5282 - precision: 0.6403 - recall: 0.0019 - auc: 0.5804\n",
      "Epoch 60/1000\n",
      "800/800 [==============================] - 90s 112ms/step - loss: 0.5238 - precision: 0.7702 - recall: 0.0027 - auc: 0.5841 - val_loss: 0.6198 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5309\n",
      "Epoch 61/1000\n",
      "800/800 [==============================] - 63s 79ms/step - loss: 0.5273 - precision: 0.6434 - recall: 0.0018 - auc: 0.5799\n",
      "Epoch 62/1000\n",
      "800/800 [==============================] - 63s 79ms/step - loss: 0.5245 - precision: 0.7261 - recall: 0.0025 - auc: 0.5856\n",
      "Epoch 63/1000\n",
      "800/800 [==============================] - 63s 79ms/step - loss: 0.5255 - precision: 0.6115 - recall: 0.0018 - auc: 0.5828\n",
      "Epoch 64/1000\n",
      "800/800 [==============================] - 63s 79ms/step - loss: 0.5259 - precision: 0.6863 - recall: 0.0023 - auc: 0.5812\n",
      "Epoch 65/1000\n",
      "800/800 [==============================] - 63s 79ms/step - loss: 0.5244 - precision: 0.6981 - recall: 0.0024 - auc: 0.5821\n",
      "Epoch 66/1000\n",
      "800/800 [==============================] - 63s 79ms/step - loss: 0.5272 - precision: 0.7012 - recall: 0.0025 - auc: 0.5806\n",
      "Epoch 67/1000\n",
      "800/800 [==============================] - 63s 79ms/step - loss: 0.5237 - precision: 0.6774 - recall: 0.0023 - auc: 0.5820\n",
      "Epoch 68/1000\n",
      "800/800 [==============================] - 63s 78ms/step - loss: 0.5284 - precision: 0.6529 - recall: 0.0024 - auc: 0.5797\n",
      "Epoch 69/1000\n",
      "800/800 [==============================] - 63s 79ms/step - loss: 0.5226 - precision: 0.6250 - recall: 0.0022 - auc: 0.5832\n",
      "Epoch 70/1000\n",
      "800/800 [==============================] - 89s 111ms/step - loss: 0.5286 - precision: 0.7037 - recall: 0.0024 - auc: 0.5807 - val_loss: 0.6321 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5229\n",
      "Epoch 71/1000\n",
      "800/800 [==============================] - 63s 79ms/step - loss: 0.5226 - precision: 0.7186 - recall: 0.0026 - auc: 0.5842\n",
      "Epoch 72/1000\n",
      "800/800 [==============================] - 63s 79ms/step - loss: 0.5285 - precision: 0.6289 - recall: 0.0021 - auc: 0.5804\n",
      "Epoch 73/1000\n",
      "800/800 [==============================] - 63s 79ms/step - loss: 0.5242 - precision: 0.6802 - recall: 0.0025 - auc: 0.5822\n",
      "Epoch 74/1000\n",
      "800/800 [==============================] - 63s 78ms/step - loss: 0.5277 - precision: 0.6732 - recall: 0.0022 - auc: 0.5804\n",
      "Epoch 75/1000\n",
      "800/800 [==============================] - 63s 78ms/step - loss: 0.5242 - precision: 0.6813 - recall: 0.0027 - auc: 0.5841\n",
      "Epoch 76/1000\n",
      "800/800 [==============================] - 63s 78ms/step - loss: 0.5257 - precision: 0.6333 - recall: 0.0021 - auc: 0.5823\n",
      "Epoch 77/1000\n",
      "800/800 [==============================] - 63s 78ms/step - loss: 0.5256 - precision: 0.7011 - recall: 0.0026 - auc: 0.5820\n",
      "Epoch 78/1000\n",
      "800/800 [==============================] - 63s 78ms/step - loss: 0.5251 - precision: 0.6646 - recall: 0.0024 - auc: 0.5808\n",
      "Epoch 79/1000\n",
      "800/800 [==============================] - 63s 79ms/step - loss: 0.5270 - precision: 0.6564 - recall: 0.0023 - auc: 0.5824\n",
      "Epoch 80/1000\n",
      "800/800 [==============================] - 89s 112ms/step - loss: 0.5233 - precision: 0.6625 - recall: 0.0023 - auc: 0.5823 - val_loss: 0.6295 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5291\n",
      "Epoch 81/1000\n",
      "800/800 [==============================] - 63s 78ms/step - loss: 0.5281 - precision: 0.7091 - recall: 0.0025 - auc: 0.5814\n",
      "Epoch 82/1000\n",
      "800/800 [==============================] - 63s 79ms/step - loss: 0.5226 - precision: 0.6485 - recall: 0.0023 - auc: 0.5830\n",
      "Epoch 83/1000\n",
      "800/800 [==============================] - 63s 78ms/step - loss: 0.5283 - precision: 0.6769 - recall: 0.0028 - auc: 0.5807\n",
      "Epoch 84/1000\n",
      "800/800 [==============================] - 63s 79ms/step - loss: 0.5227 - precision: 0.6705 - recall: 0.0026 - auc: 0.5826\n",
      "Epoch 85/1000\n",
      "800/800 [==============================] - 63s 78ms/step - loss: 0.5294 - precision: 0.6647 - recall: 0.0024 - auc: 0.5816\n",
      "Epoch 86/1000\n",
      "800/800 [==============================] - 63s 78ms/step - loss: 0.5239 - precision: 0.6667 - recall: 0.0025 - auc: 0.5824\n",
      "Epoch 87/1000\n",
      "800/800 [==============================] - 63s 78ms/step - loss: 0.5263 - precision: 0.7078 - recall: 0.0024 - auc: 0.5826\n",
      "Epoch 88/1000\n",
      "800/800 [==============================] - 63s 78ms/step - loss: 0.5242 - precision: 0.6853 - recall: 0.0029 - auc: 0.5836\n",
      "Epoch 89/1000\n",
      "800/800 [==============================] - 63s 78ms/step - loss: 0.5266 - precision: 0.6581 - recall: 0.0022 - auc: 0.5815\n",
      "Epoch 90/1000\n",
      "800/800 [==============================] - 89s 111ms/step - loss: 0.5252 - precision: 0.7022 - recall: 0.0027 - auc: 0.5818 - val_loss: 0.6225 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5290\n",
      "Epoch 91/1000\n",
      "800/800 [==============================] - 63s 78ms/step - loss: 0.5251 - precision: 0.7251 - recall: 0.0027 - auc: 0.5824\n",
      "Epoch 92/1000\n",
      "800/800 [==============================] - 63s 78ms/step - loss: 0.5272 - precision: 0.6571 - recall: 0.0025 - auc: 0.5823\n",
      "Epoch 93/1000\n",
      "800/800 [==============================] - 63s 78ms/step - loss: 0.5228 - precision: 0.7215 - recall: 0.0025 - auc: 0.5839\n",
      "Epoch 94/1000\n",
      "800/800 [==============================] - 63s 78ms/step - loss: 0.5283 - precision: 0.6831 - recall: 0.0027 - auc: 0.5803\n",
      "Epoch 95/1000\n",
      "800/800 [==============================] - 63s 79ms/step - loss: 0.5224 - precision: 0.6865 - recall: 0.0028 - auc: 0.5817\n",
      "Epoch 96/1000\n",
      "800/800 [==============================] - 63s 78ms/step - loss: 0.5283 - precision: 0.6629 - recall: 0.0025 - auc: 0.5816\n",
      "Epoch 97/1000\n",
      "800/800 [==============================] - 63s 78ms/step - loss: 0.5236 - precision: 0.7000 - recall: 0.0029 - auc: 0.5836\n",
      "Epoch 98/1000\n",
      "800/800 [==============================] - 63s 79ms/step - loss: 0.5277 - precision: 0.6914 - recall: 0.0026 - auc: 0.5817\n",
      "Epoch 99/1000\n",
      "800/800 [==============================] - 63s 78ms/step - loss: 0.5238 - precision: 0.6751 - recall: 0.0029 - auc: 0.5830\n",
      "Epoch 100/1000\n",
      "800/800 [==============================] - 89s 111ms/step - loss: 0.5280 - precision: 0.6746 - recall: 0.0024 - auc: 0.5790 - val_loss: 0.6333 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5302\n",
      "Epoch 101/1000\n",
      "800/800 [==============================] - 63s 79ms/step - loss: 0.5234 - precision: 0.7412 - recall: 0.0027 - auc: 0.5849\n",
      "Epoch 102/1000\n",
      "800/800 [==============================] - 63s 78ms/step - loss: 0.5271 - precision: 0.6897 - recall: 0.0026 - auc: 0.5809\n",
      "Epoch 103/1000\n",
      "800/800 [==============================] - 63s 78ms/step - loss: 0.5246 - precision: 0.6800 - recall: 0.0026 - auc: 0.5832\n",
      "Epoch 104/1000\n",
      "800/800 [==============================] - 63s 78ms/step - loss: 0.5260 - precision: 0.6947 - recall: 0.0029 - auc: 0.5821\n",
      "Epoch 105/1000\n",
      "800/800 [==============================] - 63s 78ms/step - loss: 0.5257 - precision: 0.6186 - recall: 0.0026 - auc: 0.5838\n",
      "Epoch 106/1000\n",
      "800/800 [==============================] - 63s 79ms/step - loss: 0.5233 - precision: 0.6927 - recall: 0.0027 - auc: 0.5834\n",
      "Epoch 107/1000\n",
      "800/800 [==============================] - 63s 78ms/step - loss: 0.5282 - precision: 0.6944 - recall: 0.0027 - auc: 0.5804\n",
      "Epoch 108/1000\n",
      "800/800 [==============================] - 63s 79ms/step - loss: 0.5228 - precision: 0.6872 - recall: 0.0027 - auc: 0.5836\n",
      "Epoch 109/1000\n",
      "800/800 [==============================] - 63s 78ms/step - loss: 0.5277 - precision: 0.6615 - recall: 0.0027 - auc: 0.5811\n",
      "Epoch 110/1000\n",
      "800/800 [==============================] - 88s 111ms/step - loss: 0.5232 - precision: 0.6536 - recall: 0.0026 - auc: 0.5826 - val_loss: 0.6293 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5294\n",
      "Epoch 111/1000\n",
      "694/800 [=========================>....] - ETA: 8s - loss: 0.5272 - precision: 0.6959 - recall: 0.0026 - auc: 0.5821"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-4e1713c97d11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m                 \u001b[0;34m,\u001b[0m\u001b[0mvalidation_freq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                 \u001b[0;34m,\u001b[0m\u001b[0mvalidation_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                 \u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mWandbCallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m                 )\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/wandb/integration/keras/keras.py\u001b[0m in \u001b[0;36mnew_v2\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcbk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcbks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0mset_wandb_attrs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcbk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mold_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0mtraining_arrays\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morig_fit_loop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run = wandb.init(project=\"candlestick-CNN\", name = 'TLShallow20FilesShuffle20K_AdamLR1e-4' ,reinit= True)\n",
    "\n",
    "history = model.fit(train\n",
    "                ,epochs=1000\n",
    "                ,steps_per_epoch=10240*20/256 #800\n",
    "                ,verbose=1\n",
    "                ,validation_data=test                \n",
    "                ,validation_freq = 10\n",
    "                ,validation_steps = 100\n",
    "                ,callbacks=[WandbCallback()]\n",
    "                )\n",
    "\n"
   ]
  }
 ]
}