{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.6.9 64-bit",
   "display_name": "Python 3.6.9 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "4cd7ab41f5fca4b9b44701077e38c5ffd31fe66a6cab21e0214b68d958d0e462"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Loss Reduction - Evaluation\n",
    "\n",
    "The past few experiments using the bigger dataset, the model was not converging. The loss was not going below 0.5, so the objective is to find a model which can give a zero loss\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "source": [
    "## Shallow CNN of 2 Layers\n",
    "\n",
    "```\n",
    "### Model Architecuture\n",
    "IMG_SIZE = 128\n",
    "ACTIVATION = 'relu'\n",
    "KERNEL_INITIALISER = 'glorot_normal'\n",
    "KERNEL_SIZE = (3,3)\n",
    "POOL_SIZE = (6,6)\n",
    "\n",
    "model = tf.keras.Sequential(name='Base')\n",
    "model.add(tf.keras.layers.Input(shape=(IMG_SIZE,IMG_SIZE,3)))\n",
    "model.add(tf.keras.layers.experimental.preprocessing.Rescaling(1./255))\n",
    "model.add(tf.keras.layers.BatchNormalization())            \n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(filters=10,kernel_size=KERNEL_SIZE,activation=ACTIVATION,kernel_initializer=KERNEL_INITIALISER,padding='same'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size = POOL_SIZE,padding = 'same'))\n",
    "model.add(tf.keras.layers.Conv2D(filters=20,kernel_size=KERNEL_SIZE,activation=ACTIVATION,kernel_initializer=KERNEL_INITIALISER,padding='same'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size = POOL_SIZE,padding = 'same'))\n",
    "\n",
    "\n",
    "model.add(tf.keras.layers.Flatten())    \n",
    "\n",
    "model.add(tf.keras.layers.Dense(64,activation=ACTIVATION,kernel_initializer=KERNEL_INITIALISER))\n",
    "model.add(tf.keras.layers.Dense(128,activation=ACTIVATION,kernel_initializer=KERNEL_INITIALISER))\n",
    "      \n",
    "model.add(tf.keras.layers.Dense(1,activation='sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Experiment : Shuffle Buffer Size and LR\n",
    "\n",
    "The data was loaded using tf.Data and different combination of shuffle() buffer sizes and learning rates were tried to see their impact on loss reduction\n",
    "\n",
    "General Notes :\n",
    "* **Learning Rate** has no impact on training time, as it is a parameter by which the model weights are adjusted even though the word 'rate' might seem to have a time dimension\n",
    "* But **Learning Rate** will have an impact on loss reduction. If LR is smaller, then it will take the GD algorithm more time to reach the loss minimum.\n",
    "* In one **epoch**, the GD algorithm forward passes through all the samples in the dataset and then backward passes to update the weights. In **mini-batch GD**, the dataset is divided into mini-batches of smaller sizes (typically 64,128,256) and it follows the same process\n",
    "* If tf.Data is used with infinite repetitions `repeat(-1)`, then `steps_per_epoch` must be specified. This should be large enough to cover the entire dataset `shuffle(buffer_size)`\n",
    "* Larger datasets take longer time to converge\n",
    "\n",
    "Observations :\n",
    "\n",
    "* LR 1e-4\n",
    "    + Loss is tending towards zero unlike previous experiments\n",
    "    + buffer_size of 40K, the model is converging but at slower rate\n",
    "    + Smaller buffer size has higher loss variations between epochs\n",
    "* Large Dataset (140K files ~ 500K samples) with Smaller buffer size (10K) gave a very wiggly loss. Lowering LR from 1e-4 to 1e-10 did not help.\n",
    "    + This happened most likely because my steps per epoch were very low [CHECK THIS]\n",
    "* Increased steps per epoch to 800 (x256) so that it covers the 200K shuffle buffer size\n",
    "    + LR of 1e-4 the model doesn't seem to converge. It went lower around epoch 65 but then started climbing again\n",
    "    + Tried with LR of 1e-10 but the loss starts around 1.05 and will take a long time to come down. So stopped\n",
    "    \n",
    "* LR 1e-5    \n",
    "    + The model with 1e-05 LR was not converging fast enough\n",
    "\n",
    "* The initial loss starts higher for lower learning rates (e.g 1e-10 started at 0.7 but 1e-10 started around 0.55) (WHY)\n",
    "\n",
    "\n",
    "To try :\n",
    "+ What happens when data was not shuffled?\n",
    "+ look at validation parameters\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "![reduce-loss-png](./evaluation-data/ReduceLoss-wanb.PNG)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Experiment : LeakyRelu and Learning Rate Decay\n",
    "\n",
    "* Initial loss started very high but then stabilised\n",
    "* After reaching around 0.6 at around 450 epochs. the loss reducing very slowly\n",
    "* The learning rate at 450 epochs was around 0.01, which was probably still high ?\n",
    "* Validation loss is averaging 0.6\n",
    "* The LR around 800 epochs is 0.001\n",
    "\n",
    "\n",
    "```\n",
    "#LR Decay settings\n",
    "initial_learning_rate = 0.1\n",
    "lr_scheduler = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=800*10, #every 10 epochs, there were 800 steps per epoch. Each step covering 256 samples (800x256 = 200K shuffle buffer_size)\n",
    "    decay_rate=0.95,\n",
    "    staircase=True)\n",
    "```\n",
    "\n",
    "![leaky relu-Initial](./evaluation-data/leaky-relu-initial-loss.png)\n",
    "![leaky relu](./evaluation-data/leaky-relu2.png)\n",
    "![leaky relu](./evaluation-data/leaky-relu3.png)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Experiment : Optimisers\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "           loss  precision    recall       auc  elapsed  \\\n",
       "epoch                                                     \n",
       "0      0.819574   0.226315  0.786526  0.485282     0.21   \n",
       "1      0.813502   0.229212  0.760717  0.478854     0.21   \n",
       "0      0.566792   0.229156  0.399007  0.495355     0.47   \n",
       "1      0.543850   0.000000  0.000000  0.510221     0.47   \n",
       "0      0.539155   0.000000  0.000000  0.523841     0.72   \n",
       "1      0.541593   0.000000  0.000000  0.532054     0.72   \n",
       "0      0.538194   0.000000  0.000000  0.541809     0.97   \n",
       "1      0.544207   0.000000  0.000000  0.529684     0.97   \n",
       "0      0.541666   0.000000  0.000000  0.538182     1.24   \n",
       "1      0.538303   0.000000  0.000000  0.529026     1.24   \n",
       "0      0.541205   0.000000  0.000000  0.543394     1.46   \n",
       "1      0.537259   0.000000  0.000000  0.539070     1.46   \n",
       "\n",
       "                                                  params  \n",
       "epoch                                                     \n",
       "0      ReduceLoss_Shallow_AllData20Files_DataShuffle2...  \n",
       "1      ReduceLoss_Shallow_AllData20Files_DataShuffle2...  \n",
       "0      ReduceLoss_Shallow_AllData20Files_DataShuffle2...  \n",
       "1      ReduceLoss_Shallow_AllData20Files_DataShuffle2...  \n",
       "0      ReduceLoss_Shallow_AllData20Files_DataShuffle2...  \n",
       "1      ReduceLoss_Shallow_AllData20Files_DataShuffle2...  \n",
       "0      ReduceLoss_Shallow_AllData20Files_DataShuffle2...  \n",
       "1      ReduceLoss_Shallow_AllData20Files_DataShuffle2...  \n",
       "0      ReduceLoss_Shallow_AllData20Files_DataShuffle2...  \n",
       "1      ReduceLoss_Shallow_AllData20Files_DataShuffle2...  \n",
       "0      ReduceLoss_Shallow_AllData20Files_DataShuffle2...  \n",
       "1      ReduceLoss_Shallow_AllData20Files_DataShuffle2...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>loss</th>\n      <th>precision</th>\n      <th>recall</th>\n      <th>auc</th>\n      <th>elapsed</th>\n      <th>params</th>\n    </tr>\n    <tr>\n      <th>epoch</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.819574</td>\n      <td>0.226315</td>\n      <td>0.786526</td>\n      <td>0.485282</td>\n      <td>0.21</td>\n      <td>ReduceLoss_Shallow_AllData20Files_DataShuffle2...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.813502</td>\n      <td>0.229212</td>\n      <td>0.760717</td>\n      <td>0.478854</td>\n      <td>0.21</td>\n      <td>ReduceLoss_Shallow_AllData20Files_DataShuffle2...</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>0.566792</td>\n      <td>0.229156</td>\n      <td>0.399007</td>\n      <td>0.495355</td>\n      <td>0.47</td>\n      <td>ReduceLoss_Shallow_AllData20Files_DataShuffle2...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.543850</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.510221</td>\n      <td>0.47</td>\n      <td>ReduceLoss_Shallow_AllData20Files_DataShuffle2...</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>0.539155</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.523841</td>\n      <td>0.72</td>\n      <td>ReduceLoss_Shallow_AllData20Files_DataShuffle2...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.541593</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.532054</td>\n      <td>0.72</td>\n      <td>ReduceLoss_Shallow_AllData20Files_DataShuffle2...</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>0.538194</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.541809</td>\n      <td>0.97</td>\n      <td>ReduceLoss_Shallow_AllData20Files_DataShuffle2...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.544207</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.529684</td>\n      <td>0.97</td>\n      <td>ReduceLoss_Shallow_AllData20Files_DataShuffle2...</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>0.541666</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.538182</td>\n      <td>1.24</td>\n      <td>ReduceLoss_Shallow_AllData20Files_DataShuffle2...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.538303</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.529026</td>\n      <td>1.24</td>\n      <td>ReduceLoss_Shallow_AllData20Files_DataShuffle2...</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>0.541205</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.543394</td>\n      <td>1.46</td>\n      <td>ReduceLoss_Shallow_AllData20Files_DataShuffle2...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.537259</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.539070</td>\n      <td>1.46</td>\n      <td>ReduceLoss_Shallow_AllData20Files_DataShuffle2...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "#Model name was not set correctly\n",
    "#df[480:]['params'].replace('Model_Base','modelBase_with_BN_Dropout',regex`=True,inplace=True)\n",
    "import pandas as pd\n",
    "df = pd.read_pickle('./evaluation-data/test-varyOptimisers.pkl', compression='infer')\n",
    "#Split the params column\n",
    "# df[['model','x','IMGs','x','LR','x','BS']] = df['params'].str.split('_',expand=True)\n",
    "# df.drop(columns = 'x',inplace = True)\n",
    "\n",
    "\n",
    "df['params'].str.split('_',expand=True)\n",
    "df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "import time\n",
    "\n",
    "from functions_dataCreation import *\n",
    "from functions_modelArchitectures import *\n",
    "\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "IMG_SIZE = 128\n",
    "\n",
    "METRICS = [\n",
    "    #   keras.metrics.TruePositives(name='tp'),\n",
    "    #   keras.metrics.FalsePositives(name='fp'),\n",
    "    #   keras.metrics.TrueNegatives(name='tn'),\n",
    "    #   keras.metrics.FalseNegatives(name='fn'), \n",
    "    #   keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      keras.metrics.Precision(name='precision'),\n",
    "      keras.metrics.Recall(name='recall'),\n",
    "      keras.metrics.AUC(name='auc'),\n",
    "]\n"
   ]
  },
  {
   "source": [
    "### The datasets will not be shuffled"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = createIODataset(140,'../data/Train')\n",
    "test = createIODataset(4,'../data/Test')\n",
    "\n",
    "train = train.repeat(-1)\n",
    "# train = train.shuffle(buffer_size=10240*20,reshuffle_each_iteration=True)\n",
    "train = train.batch(256,drop_remainder=True)\n",
    "train = train.prefetch(10)\n",
    "\n",
    "test = test.repeat(-1)\n",
    "# test = test.shuffle(buffer_size=10240,reshuffle_each_iteration=True)\n",
    "test = test.batch(1024,drop_remainder=True)\n",
    "test = test.prefetch(10)\n",
    "\n"
   ]
  },
  {
   "source": [
    "### Model Architecture"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"Base\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nrescaling_8 (Rescaling)      (None, 128, 128, 3)       0         \n_________________________________________________________________\nbatch_normalization_6 (Batch (None, 128, 128, 3)       12        \n_________________________________________________________________\nconv2d_8 (Conv2D)            (None, 128, 128, 10)      280       \n_________________________________________________________________\nmax_pooling2d_7 (MaxPooling2 (None, 22, 22, 10)        0         \n_________________________________________________________________\nconv2d_9 (Conv2D)            (None, 22, 22, 20)        1820      \n_________________________________________________________________\nmax_pooling2d_8 (MaxPooling2 (None, 4, 4, 20)          0         \n_________________________________________________________________\nflatten_4 (Flatten)          (None, 320)               0         \n_________________________________________________________________\ndense_12 (Dense)             (None, 64)                20544     \n_________________________________________________________________\ndense_13 (Dense)             (None, 128)               8320      \n_________________________________________________________________\ndense_14 (Dense)             (None, 1)                 129       \n=================================================================\nTotal params: 31,105\nTrainable params: 31,099\nNon-trainable params: 6\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "ACTIVATION = 'relu'\n",
    "KERNEL_INITIALISER = 'glorot_normal'\n",
    "KERNEL_SIZE = (3,3)\n",
    "POOL_SIZE = (6,6)\n",
    "# model.add(tf.keras.layers.Dropout(DROPOUT_RATE))\n",
    "\n",
    "model = tf.keras.Sequential(name='Base')\n",
    "model.add(tf.keras.layers.Input(shape=(IMG_SIZE,IMG_SIZE,3)))\n",
    "model.add(tf.keras.layers.experimental.preprocessing.Rescaling(1./255))\n",
    "model.add(tf.keras.layers.BatchNormalization())            \n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(filters=10,kernel_size=KERNEL_SIZE,activation=ACTIVATION,kernel_initializer=KERNEL_INITIALISER,padding='same'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size = POOL_SIZE,padding = 'same'))\n",
    "model.add(tf.keras.layers.Conv2D(filters=20,kernel_size=KERNEL_SIZE,activation=ACTIVATION,kernel_initializer=KERNEL_INITIALISER,padding='same'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size = POOL_SIZE,padding = 'same'))\n",
    "\n",
    "\n",
    "model.add(tf.keras.layers.Flatten())    \n",
    "\n",
    "model.add(tf.keras.layers.Dense(64,activation=ACTIVATION,kernel_initializer=KERNEL_INITIALISER))\n",
    "model.add(tf.keras.layers.Dense(128,activation=ACTIVATION,kernel_initializer=KERNEL_INITIALISER))\n",
    "      \n",
    "model.add(tf.keras.layers.Dense(1,activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-7\n",
    "\n",
    "wandb.init(project=\"candlestick-CNN\", name = 'ReduceLoss_Shallow_tfDataNOShuffle_' + str(LR) )\n",
    "\n",
    "df = pd.DataFrame()\n",
    "start_time = time.time()\n",
    "\n",
    "checkpoint_path = '../data/callbacks/'\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, verbose=1, save_weights_only=True,period=500)   \n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=LR)\n",
    "                    ,loss=tf.keras.losses.binary_crossentropy\n",
    "                    ,metrics=[METRICS])\n",
    "\n",
    "history = model.fit(train\n",
    "                #,batch_size = 128\n",
    "                ,epochs=5000\n",
    "                ,steps_per_epoch=100\n",
    "                ,verbose=0\n",
    "                ,validation_data=test                \n",
    "                ,validation_freq = 100\n",
    "                ,validation_steps = 10\n",
    "                ,callbacks=[WandbCallback(),cp_callback]\n",
    "                )\n",
    "\n",
    "temp = pd.DataFrame(history.history).rename_axis(\"epoch\")\n",
    "temp['elapsed'] = round((time.time() - start_time)/60,2)\n",
    "var_params = \"Shallow_\" + \"_LR_\"  + str(LR)\n",
    "temp['params'] = var_params\n",
    "\n",
    "print(\"Elapsed time \" + str(round((time.time() - start_time)/60,2)) + var_params)\n",
    "\n",
    "# model.save('../data/savedmodels') \n",
    "\n"
   ]
  },
  {
   "source": [
    "## Faster LR\n",
    " \n",
    "* Train a CNN for 10000 epochs with LR of 1e-4\n",
    "* Changed glorot_normal for relu activation\n",
    "* Added BN for first image layer\n",
    "\n",
    "For Batch Size of 256, to complete 1 epoch the model take 2114 steps. So total dataset size is approx 500K images. Time taken per epoch is 198s = 3.5mins\n",
    "\n",
    "Lets do 200 steps per epoch i.e 51200 (samples) ie 10% of total per epoch\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Failed to query for notebook name, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mamitagni\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.9 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                Tracking run with wandb version 0.10.8<br/>\n                Syncing run <strong style=\"color:#cdcd00\">Deeper tfData 1e-6 100steps per epoch shuffle size 200K Deeper CNN NO Shuffle</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://wandb.ai/amitagni/candlestick-CNN\" target=\"_blank\">https://wandb.ai/amitagni/candlestick-CNN</a><br/>\n                Run page: <a href=\"https://wandb.ai/amitagni/candlestick-CNN/runs/vksvhz3c\" target=\"_blank\">https://wandb.ai/amitagni/candlestick-CNN/runs/vksvhz3c</a><br/>\n                Run data is saved locally in <code>wandb/run-20201109_113426-vksvhz3c</code><br/><br/>\n            "
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "\n",
      "Epoch 00500: saving model to ../data/callbacks/\n",
      "\n",
      "Epoch 01000: saving model to ../data/callbacks/\n",
      "\n",
      "Epoch 01500: saving model to ../data/callbacks/\n",
      "\n",
      "Epoch 02000: saving model to ../data/callbacks/\n",
      "\n",
      "Epoch 02500: saving model to ../data/callbacks/\n",
      "\n",
      "Epoch 03000: saving model to ../data/callbacks/\n",
      "\n",
      "Epoch 03500: saving model to ../data/callbacks/\n",
      "\n",
      "Epoch 04000: saving model to ../data/callbacks/\n",
      "\n",
      "Epoch 04500: saving model to ../data/callbacks/\n",
      "\n",
      "Epoch 05000: saving model to ../data/callbacks/\n"
     ]
    }
   ],
   "source": [
    "\n",
    "wandb.init(project=\"candlestick-CNN\", name = 'Deeper tfData 1e-7 100steps per epoch shuffle size 200K Deeper CNN NO Shuffle' )\n",
    "\n",
    "modelBase = {}\n",
    "modelBase['name'] = 'Base'\n",
    "modelBase['inputShape'] = (IMG_SIZE,IMG_SIZE,3)\n",
    "modelBase['activation'] = 'relu'\n",
    "\n",
    "modelBase['convLayerMultiplier'] = 1\n",
    "\n",
    "modelBase['poolingLayer'] = 'MaxPooling2D'\n",
    "modelBase['padding'] = 'same'\n",
    "\n",
    "modelBase['denseLayers'] = 2\n",
    "modelBase['units'] = 128\n",
    "modelBase['activation'] = 'relu'\n",
    "\n",
    "#with Dropout and BN\n",
    "modelBase_with_Dropout = modelBase.copy()\n",
    "modelBase_with_Dropout['name'] = 'modelBase_with_Dropout'\n",
    "modelBase_with_Dropout['batchnormalization'] = False\n",
    "# modelBase_with_Dropout['dropout'] = 0.00001\n",
    "\n",
    "modelBase_with_Dropout['kernelSize'] = (3,3)\n",
    "modelBase_with_Dropout['filters'] = [10,15,20,25,30,35,40,45,50,55,60]\n",
    "modelBase_with_Dropout['poolSize'] = (6,6)\n",
    "\n",
    "lr = 1e-7\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "checkpoint_path = '../data/callbacks/'\n",
    "\n",
    "# Create a callback that saves the model's weights every 5 epochs\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path, \n",
    "    verbose=1, \n",
    "    save_weights_only=True,\n",
    "    period=500)   \n",
    "\n",
    "model = createCNN(modelBase_with_Dropout)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=lr)\n",
    "        ,loss=tf.keras.losses.binary_crossentropy\n",
    "        ,metrics=[METRICS])\n",
    "\n",
    "history = model.fit(train\n",
    "                #,batch_size = 128\n",
    "                ,epochs=5000\n",
    "                ,steps_per_epoch=100\n",
    "                ,verbose=0\n",
    "                ,validation_data=test                \n",
    "                ,validation_freq = 100\n",
    "                ,validation_steps = 10\n",
    "                ,callbacks=[WandbCallback(),cp_callback]\n",
    "                )\n",
    "\n",
    "# temp = pd.DataFrame(history.history).rename_axis(\"epoch\")\n",
    "# temp['elapsed'] = round((time.time() - start_time)/60,2)\n",
    "# var_params = \"Deeper_\" + \"_LR_\"  + str(lr)\n",
    "# temp['params'] = var_params\n",
    "\n",
    "# print(\"Elapsed time \" + str(round((time.time() - start_time)/60,2)) + var_params)\n",
    "\n",
    "# model.save('../data/savedmodels') \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "614400"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "1024*100*6"
   ]
  }
 ]
}